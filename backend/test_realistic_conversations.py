#!/usr/bin/env python3
"""
ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ë° ë¡œê·¸ ë¶„ì„ ë„êµ¬

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‹¤ì œ ì‚¬ìš©ì ëŒ€í™” íŒ¨í„´ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  
ëŒ€í™” ë¡œê·¸ë¥¼ ì €ì¥í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import datetime
from pathlib import Path
from typing import List, Dict, Any
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from langchain_core.messages import HumanMessage, AIMessage
from app.graph.agent import factual_answer_node, web_search_node
from app.services.rag_service import rag_service


class ConversationLogger:
    """ëŒ€í™” ë¡œê·¸ë¥¼ ì €ì¥í•˜ê³  ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, log_dir: str = "conversation_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.conversations = []
        
    def log_conversation(self, scenario: str, user_input: str, agent_response: str, 
                        metadata: Dict[str, Any] = None):
        """ëŒ€í™” ë¡œê·¸ ì €ì¥"""
        conversation_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "scenario": scenario,
            "user_input": user_input,
            "agent_response": agent_response,
            "metadata": metadata or {}
        }
        self.conversations.append(conversation_entry)
        
    def save_logs(self, filename: str = None):
        """ë¡œê·¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_log_{timestamp}.json"
            
        filepath = self.log_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.conversations, f, ensure_ascii=False, indent=2)
            
        return filepath


class RealisticConversationTester:
    """ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = ConversationLogger()
        self.scenarios = self._load_test_scenarios()
        
    def _load_test_scenarios(self) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ"""
        return [
            {
                "name": "ë””ë”¤ëŒ_ê¸°ë³¸ì •ë³´_ë¬¸ì˜",
                "category": "didimdol_basic",
                "conversations": [
                    {
                        "user": "ì•ˆë…•í•˜ì„¸ìš”, ë””ë”¤ëŒ ëŒ€ì¶œì— ëŒ€í•´ ì•Œê³  ì‹¶ì–´ìš”",
                        "expected_topics": ["ë””ë”¤ëŒ", "ì²­ë…„", "ëŒ€ì¶œ", "ì£¼íƒ"]
                    },
                    {
                        "user": "ë””ë”¤ëŒ ëŒ€ì¶œì´ ë­”ê°€ìš”? ì²˜ìŒ ë“¤ì–´ë³´ëŠ”ë°ìš”",
                        "expected_topics": ["ë””ë”¤ëŒ", "ì •ë¶€ì§€ì›", "ìƒì• ìµœì´ˆ"]
                    }
                ]
            },
            {
                "name": "ë””ë”¤ëŒ_ê¸ˆë¦¬_ë¬¸ì˜",
                "category": "didimdol_interest",
                "conversations": [
                    {
                        "user": "ë””ë”¤ëŒ ëŒ€ì¶œ ê¸ˆë¦¬ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                        "expected_topics": ["ê¸ˆë¦¬", "%", "ì—°"]
                    },
                    {
                        "user": "ë‹¤ë¥¸ ì€í–‰ ëŒ€ì¶œì´ë‘ ê¸ˆë¦¬ ë¹„êµí•˜ë©´ ì–´ë–¤ê°€ìš”?",
                        "expected_topics": ["ê¸ˆë¦¬", "ë¹„êµ", "ì‹œì¤‘ì€í–‰"]
                    }
                ]
            },
            {
                "name": "ì „ì„¸ìê¸ˆ_ê¸‰í•œìƒí™©",
                "category": "jeonse_urgent",
                "conversations": [
                    {
                        "user": "ë‹¤ìŒ ì£¼ì— ì „ì„¸ ê³„ì•½í•´ì•¼ í•˜ëŠ”ë° ì „ì„¸ìê¸ˆëŒ€ì¶œ ë°›ì„ ìˆ˜ ìˆì„ê¹Œìš”?",
                        "expected_topics": ["ì „ì„¸ìê¸ˆëŒ€ì¶œ", "ê³„ì•½", "ê¸°ê°„"]
                    }
                ]
            },
            {
                "name": "ê°ì •ì _ìƒí™©",
                "category": "emotional",
                "conversations": [
                    {
                        "user": "ëŒ€ì¶œ ì‹¬ì‚¬ì—ì„œ ë–¨ì–´ì¡Œì–´ìš”... ë„ˆë¬´ ì†ìƒí•´ìš”. ë‹¤ë¥¸ ë°©ë²• ì—†ì„ê¹Œìš”?",
                        "expected_topics": ["ì´í•´", "ë°©ë²•", "ëŒ€ì•ˆ"]
                    }
                ]
            },
            {
                "name": "ë³µí•©_ë¬¸ì˜",
                "category": "multi_product",
                "conversations": [
                    {
                        "user": "ë””ë”¤ëŒ ëŒ€ì¶œì´ë‘ ì „ì„¸ìê¸ˆëŒ€ì¶œ ë‘˜ ë‹¤ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
                        "expected_topics": ["ë””ë”¤ëŒ", "ì „ì„¸ìê¸ˆ", "ì¤‘ë³µ"]
                    }
                ]
            }
        ]
    
    async def test_conversation(self, scenario: Dict[str, Any], conversation: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ëŒ€í™” í…ŒìŠ¤íŠ¸"""
        user_input = conversation["user"]
        expected_topics = conversation["expected_topics"]
        
        # ì—ì´ì „íŠ¸ ìƒíƒœ ì„¤ì •
        state = {
            "user_input_text": user_input,
            "messages": [HumanMessage(content=user_input)],
            "stt_result": user_input,
            "action_plan": ["invoke_qa_agent"],
            "action_plan_struct": [{"tool": "invoke_qa_agent"}]
        }
        
        try:
            # RAG ì„œë¹„ìŠ¤ ì¤€ë¹„ í™•ì¸
            if not rag_service.is_ready():
                print("Warning: RAG service not ready, using fallback response")
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤."
                response_time = 0
            else:
                # ì‹¤ì œ ì—ì´ì „íŠ¸ í˜¸ì¶œ
                start_time = asyncio.get_event_loop().time()
                result = await factual_answer_node(state)
                end_time = asyncio.get_event_loop().time()
                response_time = end_time - start_time
                response = result.get("factual_response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì‘ë‹µ í’ˆì§ˆ ë¶„ì„
            quality_analysis = self._analyze_response_quality(response, expected_topics, user_input)
            
            # ë¡œê·¸ ì €ì¥
            metadata = {
                "response_time": response_time,
                "quality_analysis": quality_analysis,
                "expected_topics": expected_topics,
                "scenario_category": scenario["category"]
            }
            
            self.logger.log_conversation(
                scenario["name"], 
                user_input, 
                response, 
                metadata
            )
            
            return {
                "success": True,
                "user_input": user_input,
                "agent_response": response,
                "response_time": response_time,
                "quality_analysis": quality_analysis
            }
            
        except Exception as e:
            error_response = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.log_conversation(
                scenario["name"], 
                user_input, 
                error_response, 
                {"error": str(e), "scenario_category": scenario["category"]}
            )
            
            return {
                "success": False,
                "user_input": user_input,
                "agent_response": error_response,
                "error": str(e)
            }
    
    def _analyze_response_quality(self, response: str, expected_topics: List[str], user_input: str) -> Dict[str, Any]:
        """ì‘ë‹µ í’ˆì§ˆ ë¶„ì„"""
        analysis = {
            "length": len(response),
            "politeness_score": 0,
            "topic_coverage": 0,
            "completeness_score": 0,
            "found_topics": [],
            "polite_markers": []
        }
        
        # ì •ì¤‘í•¨ ì²´í¬
        polite_markers = ['ìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ì„¸ìš”', 'ì‹­ë‹ˆë‹¤', 
                         'ìš”', 'í•´ìš”', 'ë“œë ¤ìš”', 'ë“œë¦½ë‹ˆë‹¤', 'í•´ë“œë ¤ìš”']
        found_polite = [marker for marker in polite_markers if marker in response]
        analysis["polite_markers"] = found_polite
        analysis["politeness_score"] = 1 if found_polite else 0
        
        # ì£¼ì œ ì»¤ë²„ë¦¬ì§€ ì²´í¬
        found_topics = [topic for topic in expected_topics if topic in response]
        analysis["found_topics"] = found_topics
        analysis["topic_coverage"] = len(found_topics) / len(expected_topics) if expected_topics else 0
        
        # ì™„ì „ì„± ì ìˆ˜ (ê¸¸ì´ì™€ ë‚´ìš© ê¸°ë°˜)
        if len(response) >= 30 and analysis["politeness_score"] > 0:
            analysis["completeness_score"] = 0.8 + (analysis["topic_coverage"] * 0.2)
        else:
            analysis["completeness_score"] = 0.3
            
        return analysis
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ¯ ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print(f"ğŸ“‹ ì´ {len(self.scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤, {sum(len(s['conversations']) for s in self.scenarios)}ê°œ ëŒ€í™” í…ŒìŠ¤íŠ¸")
        print("-" * 80)
        
        all_results = []
        total_conversations = 0
        successful_conversations = 0
        
        for scenario in self.scenarios:
            print(f"\nğŸ”„ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
            scenario_results = []
            
            for i, conversation in enumerate(scenario["conversations"], 1):
                print(f"  ğŸ’¬ ëŒ€í™” {i}: {conversation['user'][:50]}...")
                
                result = await self.test_conversation(scenario, conversation)
                scenario_results.append(result)
                total_conversations += 1
                
                if result["success"]:
                    successful_conversations += 1
                    quality = result["quality_analysis"]
                    print(f"     âœ… ì„±ê³µ (ì‘ë‹µì‹œê°„: {result['response_time']:.2f}s)")
                    print(f"     ğŸ“Š í’ˆì§ˆ: ì •ì¤‘í•¨ {quality['politeness_score']}, ì£¼ì œì»¤ë²„ë¦¬ì§€ {quality['topic_coverage']:.1%}")
                else:
                    print(f"     âŒ ì‹¤íŒ¨: {result['error']}")
                
                # ì‘ë‹µ ì¶œë ¥ (ì²˜ìŒ 100ì)
                response_preview = result["agent_response"][:100]
                print(f"     ğŸ’­ ì‘ë‹µ: {response_preview}...")
                
            all_results.append({
                "scenario": scenario,
                "results": scenario_results
            })
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        success_rate = successful_conversations / total_conversations if total_conversations > 0 else 0
        
        summary = {
            "total_conversations": total_conversations,
            "successful_conversations": successful_conversations,
            "success_rate": success_rate,
            "timestamp": datetime.datetime.now().isoformat(),
            "detailed_results": all_results
        }
        
        return summary


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ  ë””ë”¤ëŒ ìŒì„± ì—ì´ì „íŠ¸ - ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  Warning: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
    
    tester = RealisticConversationTester()
    
    try:
        # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œë„
        print("ğŸ”§ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        if hasattr(rag_service, 'initialize'):
            await rag_service.initialize()
            
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await tester.run_all_tests()
        
        # ë¡œê·¸ ì €ì¥
        log_file = tester.logger.save_logs()
        print(f"\nğŸ’¾ ëŒ€í™” ë¡œê·¸ ì €ì¥: {log_file}")
        
        # ê²°ê³¼ ë¶„ì„ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„")
        print("=" * 80)
        
        print(f"ì „ì²´ ëŒ€í™”: {results['total_conversations']}ê°œ")
        print(f"ì„±ê³µ ëŒ€í™”: {results['successful_conversations']}ê°œ")
        print(f"ì„±ê³µë¥ : {results['success_rate']:.1%}")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
        print("\nğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒì„¸ ë¶„ì„:")
        for scenario_result in results["detailed_results"]:
            scenario_name = scenario_result["scenario"]["name"]
            scenario_results = scenario_result["results"]
            scenario_success = sum(1 for r in scenario_results if r["success"])
            scenario_total = len(scenario_results)
            
            print(f"\n  ğŸ“Œ {scenario_name}")
            print(f"     ì„±ê³µë¥ : {scenario_success}/{scenario_total} ({scenario_success/scenario_total:.1%})")
            
            # í’ˆì§ˆ ë¶„ì„
            successful_results = [r for r in scenario_results if r["success"]]
            if successful_results:
                avg_politeness = sum(r["quality_analysis"]["politeness_score"] for r in successful_results) / len(successful_results)
                avg_coverage = sum(r["quality_analysis"]["topic_coverage"] for r in successful_results) / len(successful_results)
                avg_response_time = sum(r["response_time"] for r in successful_results) / len(successful_results)
                
                print(f"     í‰ê·  ì •ì¤‘í•¨: {avg_politeness:.1%}")
                print(f"     í‰ê·  ì£¼ì œì»¤ë²„ë¦¬ì§€: {avg_coverage:.1%}")
                print(f"     í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.2f}ì´ˆ")
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥
        analysis_file = tester.log_dir / f"analysis_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ˆ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {analysis_file}")
        
        return results
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    asyncio.run(main())
# backend/app/graph/agent.py
import json
import yaml
import asyncio
from pathlib import Path
from typing import Dict, Optional, Sequence, Literal, Any, List, Union, cast, AsyncGenerator
import traceback

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from .state import AgentState, ScenarioAgentOutput, PRODUCT_TYPES
from ..core.config import OPENAI_API_KEY, LLM_MODEL_NAME
from .models import (
    next_stage_decision_parser,
    initial_task_decision_parser,
    main_router_decision_parser,
    ActionModel,
    expanded_queries_parser
)
from .utils import (
    ALL_PROMPTS,
    ALL_SCENARIOS_DATA,
    get_active_scenario_data,
    load_knowledge_base_content_async,
    format_messages_for_prompt,
    format_transitions_for_prompt,
)
from .chains import (
    json_llm,
    generative_llm,
    synthesizer_chain,
    invoke_scenario_agent_logic
)
import re
from ..services.rag_service import rag_service
from ..services.web_search_service import web_search_service

# --- Flow Tracking ---

def log_node_execution(node_name: str, input_info: str = "", output_info: str = ""):
    """Í∞ÑÍ≤∞Ìïú ÎÖ∏Îìú Ïã§Ìñâ Ï∂îÏ†Å Î°úÍπÖ"""
    if input_info and output_info:
        print(f"üîÑ [{node_name}] {input_info} ‚Üí {output_info}")
    elif input_info:
        print(f"üîÑ [{node_name}] {input_info}")
    else:
        print(f"üîÑ [{node_name}]")

# --- Helper Functions for Information Collection ---

# ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ï∂îÏ∂ú Î°úÏßÅ Ï†úÍ±∞ - Entity Agent ÏÇ¨Ïö©ÏúºÎ°ú ÎåÄÏ≤¥

def check_required_info_completion(collected_info: Dict, required_fields: List[Dict]) -> tuple[bool, List[str]]:
    """ÌïÑÏàò Ï†ïÎ≥¥ ÏàòÏßë ÏôÑÎ£å Ïó¨Î∂Ä ÌôïÏù∏"""
    missing_fields = []
    
    for field in required_fields:
        if field["required"] and field["key"] not in collected_info:
            missing_fields.append(field["display_name"])
    
    is_complete = len(missing_fields) == 0
    return is_complete, missing_fields

def generate_missing_info_prompt(missing_fields: List[str], collected_info: Dict) -> str:
    """Î∂ÄÏ°±Ìïú Ï†ïÎ≥¥Ïóê ÎåÄÌïú ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏöîÏ≤≠ Î©îÏãúÏßÄ ÏÉùÏÑ±"""
    if len(missing_fields) == 1:
        return f"{missing_fields[0]}Ïóê ÎåÄÌï¥ÏÑú ÏïåÎ†§Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"
    elif len(missing_fields) == 2:
        return f"{missing_fields[0]}Í≥º(ÏôÄ) {missing_fields[1]}Ïóê ÎåÄÌï¥ÏÑú ÏïåÎ†§Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"
    else:
        field_list = ", ".join(missing_fields[:-1])
        return f"{field_list}, Í∑∏Î¶¨Í≥† {missing_fields[-1]}Ïóê ÎåÄÌï¥ÏÑú ÏïåÎ†§Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"

def get_next_missing_info_group_stage(collected_info: Dict, required_fields: List[Dict]) -> str:
    """ÏàòÏßëÎêú Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Îã§ÏùåÏóê Î¨ºÏñ¥Î≥º Í∑∏Î£π Ïä§ÌÖåÏù¥ÏßÄ Í≤∞Ï†ï"""
    # Í∑∏Î£πÎ≥Ñ Ï†ïÎ≥¥ ÌôïÏù∏
    group1_fields = ["loan_purpose_confirmed", "marital_status"]
    group2_fields = ["has_home", "annual_income"] 
    group3_fields = ["target_home_price"]
    
    print(f"ÌòÑÏû¨ ÏàòÏßëÎêú Ï†ïÎ≥¥: {collected_info}")
    
    # Í∞Å Í∑∏Î£πÏóêÏÑú ÎàÑÎùΩÎêú Ï†ïÎ≥¥Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
    group1_missing = any(field not in collected_info for field in group1_fields)
    group2_missing = any(field not in collected_info for field in group2_fields)
    group3_missing = any(field not in collected_info for field in group3_fields)
    
    print(f"Í∑∏Î£πÎ≥Ñ ÎàÑÎùΩ ÏÉÅÌÉú - Group1: {group1_missing}, Group2: {group2_missing}, Group3: {group3_missing}")
    
    if group1_missing:
        return "ask_missing_info_group1"
    elif group2_missing:
        return "ask_missing_info_group2"
    elif group3_missing:
        return "ask_missing_info_group3"
    else:
        return "eligibility_assessment"

def generate_group_specific_prompt(stage_id: str, collected_info: Dict) -> str:
    """Í∑∏Î£πÎ≥ÑÎ°ú Ïù¥ÎØ∏ ÏàòÏßëÎêú Ï†ïÎ≥¥Î•º Ï†úÏô∏ÌïòÍ≥† ÎßûÏ∂§Ìòï ÏßàÎ¨∏ ÏÉùÏÑ±"""
    print(f"ÏßàÎ¨∏ ÏÉùÏÑ± - stage_id: {stage_id}, collected_info: {collected_info}")
    
    if stage_id == "ask_missing_info_group1":
        missing = []
        has_loan_purpose = collected_info.get("loan_purpose_confirmed", False)
        has_marital_status = "marital_status" in collected_info
        
        if not has_loan_purpose:
            missing.append("ÎåÄÏ∂ú Î™©Ï†Å(Ï£ºÌÉù Íµ¨ÏûÖÏö©Ïù∏ÏßÄ)")
        if not has_marital_status:
            missing.append("ÌòºÏù∏ ÏÉÅÌÉú")
        
        print(f"Group1 ÎàÑÎùΩ Ï†ïÎ≥¥: {missing}")
        
        if len(missing) == 2:
            return "Î™á Í∞ÄÏßÄ Îçî ÌôïÏù∏Ìï¥Î≥ºÍ≤åÏöî. ÎåÄÏ∂ú Î™©Ï†ÅÍ≥º ÌòºÏù∏ ÏÉÅÌÉúÎäî Ïñ¥ÎñªÍ≤å ÎêòÏãúÎÇòÏöî?"
        elif "ÎåÄÏ∂ú Î™©Ï†Å(Ï£ºÌÉù Íµ¨ÏûÖÏö©Ïù∏ÏßÄ)" in missing:
            return "ÎåÄÏ∂ú Î™©Ï†ÅÏùÑ ÌôïÏù∏Ìï¥Î≥ºÍ≤åÏöî. Ï£ºÌÉù Íµ¨ÏûÖ Î™©Ï†ÅÏù¥ ÎßûÏúºÏã†Í∞ÄÏöî?"
        elif "ÌòºÏù∏ ÏÉÅÌÉú" in missing:
            return "ÌòºÏù∏ ÏÉÅÌÉúÎäî Ïñ¥ÎñªÍ≤å ÎêòÏãúÎÇòÏöî? (ÎØ∏Ìòº/Í∏∞Ìòº/ÏòàÎπÑÎ∂ÄÎ∂Ä)"
        else:
            # Group1Ïùò Î™®Îì† Ï†ïÎ≥¥Í∞Ä ÏàòÏßëÎêú Í≤ΩÏö∞ Group2Î°ú ÎÑòÏñ¥Í∞ÄÏïº Ìï®
            return "Ï∂îÍ∞Ä Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"
            
    elif stage_id == "ask_missing_info_group2":
        missing = []
        if "has_home" not in collected_info:
            missing.append("Ï£ºÌÉù ÏÜåÏú† Ïó¨Î∂Ä")
        if "annual_income" not in collected_info:
            missing.append("Ïó∞ÏÜåÎìù")
            
        if len(missing) == 2:
            return "ÌòÑÏû¨ Ï£ºÌÉù ÏÜåÏú† Ïó¨Î∂ÄÏôÄ Ïó∞ÏÜåÎìùÏùÄ Ïñ¥Îäê Ï†ïÎèÑ ÎêòÏãúÎÇòÏöî?"
        elif "Ï£ºÌÉù ÏÜåÏú† Ïó¨Î∂Ä" in missing:
            return "ÌòÑÏû¨ ÏÜåÏú†ÌïòÍ≥† Í≥ÑÏã† Ï£ºÌÉùÏù¥ ÏûàÏúºÏã†Í∞ÄÏöî?"
        else:
            return "Ïó∞ÏÜåÎìùÏùÄ Ïñ¥Îäê Ï†ïÎèÑ ÎêòÏãúÎÇòÏöî? (ÏÑ∏Ï†Ñ Í∏∞Ï§Ä)"
            
    elif stage_id == "ask_missing_info_group3":
        return "Íµ¨Îß§ ÏòàÏ†ïÏù¥Ïã† Ï£ºÌÉù Í∞ÄÍ≤©ÏùÄ Ïñ¥Îäê Ï†ïÎèÑÎ°ú ÏÉùÍ∞ÅÌïòÍ≥† Í≥ÑÏã†Í∞ÄÏöî?"
    
    return "Ï∂îÍ∞Ä Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"

# --- LangGraph Node Functions ---

async def entry_point_node(state: AgentState) -> AgentState:
    user_text = state.get("user_input_text", "")
    product = state.get("current_product_type", "None")
    log_node_execution("Entry", f"input='{user_text[:20]}...', product={product}")
    if not ALL_SCENARIOS_DATA or not ALL_PROMPTS:
        error_msg = "Service initialization failed (Cannot load scenarios or prompts)."
        return {**state, "error_message": error_msg, "final_response_text_for_tts": error_msg, "is_final_turn_response": True}

    # Reset turn-specific state
    turn_defaults = {
        "stt_result": None, "main_agent_routing_decision": None, "main_agent_direct_response": None,
        "scenario_agent_output": None, "final_response_text_for_tts": None,
        "is_final_turn_response": False, "error_message": None,
        "active_scenario_data": None, "active_knowledge_base_content": None,
        "loan_selection_is_fresh": False, "factual_response": None, "action_plan": [],
    }
    
    current_product = state.get("current_product_type")
    updated_state = {**state, **turn_defaults, "current_product_type": current_product}
    
    # Load active scenario data if a product is selected
    active_scenario = get_active_scenario_data(updated_state)
    if active_scenario:
        updated_state["active_scenario_data"] = active_scenario
        updated_state["active_scenario_name"] = active_scenario.get("scenario_name", "Unknown Product")
        if not updated_state.get("current_scenario_stage_id"):
            updated_state["current_scenario_stage_id"] = active_scenario.get("initial_stage_id")
    else:
        updated_state["active_scenario_name"] = "Not Selected"

    # Add user input to message history
    user_text = updated_state.get("user_input_text")
    if user_text:
        messages = list(updated_state.get("messages", []))
        if not messages or not (isinstance(messages[-1], HumanMessage) and messages[-1].content == user_text):
            messages.append(HumanMessage(content=user_text))
        updated_state["messages"] = messages
        updated_state["stt_result"] = user_text
    
    # ÏãúÎÇòÎ¶¨Ïò§ ÏûêÎèô ÏßÑÌñâ Î°úÏßÅ
    scenario_continuation = _check_scenario_continuation(state, updated_state)
    if scenario_continuation:
        updated_state.update(scenario_continuation)
        
    return cast(AgentState, updated_state)

def _check_scenario_continuation(prev_state: AgentState, current_state: AgentState) -> dict:
    """ÏãúÎÇòÎ¶¨Ïò§ Ïó∞ÏÜç ÏßÑÌñâÏù¥ ÌïÑÏöîÌïúÏßÄ ÌôïÏù∏ÌïòÍ≥† ÏûêÎèô ÏÑ§Ï†ï"""
    
    # Ïù¥Ï†Ñ ÏÉÅÌÉúÏóêÏÑú ÏãúÎÇòÎ¶¨Ïò§ Ïó∞ÏÜçÏÑ±Ïù¥ Ï§ÄÎπÑÎêòÏñ¥ ÏûàÍ≥†, ÌòÑÏû¨ ÏÇ¨Ïö©Ïûê ÏûÖÎ†•Ïù¥ ÏûàÎäî Í≤ΩÏö∞
    if (prev_state.get("scenario_ready_for_continuation") and 
        prev_state.get("current_product_type") and 
        current_state.get("user_input_text")):
        
        print("üîÑ ÏãúÎÇòÎ¶¨Ïò§ ÏûêÎèô ÏßÑÌñâ Î™®Îìú ÌôúÏÑ±Ìôî")
        print(f"   Ï†úÌíà: {prev_state.get('current_product_type')}")
        print(f"   ÏãúÎÇòÎ¶¨Ïò§: {prev_state.get('active_scenario_name')}")
        
        return {
            "action_plan": ["invoke_scenario_agent"],
            "scenario_ready_for_continuation": False,  # ÏûêÎèô ÏßÑÌñâ ÌõÑ Î¶¨ÏÖã
            "scenario_awaiting_user_response": False,
            # Ïù¥Ï†Ñ ÏÉÅÌÉúÏóêÏÑú ÌïÑÏöîÌïú Ï†ïÎ≥¥ Î≥µÏõê
            "current_product_type": prev_state.get("current_product_type"),
            "current_scenario_stage_id": prev_state.get("current_scenario_stage_id"),
            "collected_product_info": prev_state.get("collected_product_info", {})
        }
    
    return {}

async def main_agent_router_node(state: AgentState) -> AgentState:
    user_input = state.get("stt_result", "")
    current_product_type = state.get("current_product_type")
    mode = "business_guidance" if not current_product_type else "task_management"
    log_node_execution("Orchestrator", f"mode={mode}, input='{user_input[:20]}...'")
    if not json_llm:
        return {**state, "error_message": "Orchestrator service unavailable (LLM not initialized).", "is_final_turn_response": True}

    user_input = state.get("stt_result", "")
    current_product_type = state.get("current_product_type")
    
    # LLM Í∏∞Î∞ò ÎåÄÌôî Ï≤òÎ¶¨ Î∞è Worker Í≤∞Ï†ï
    prompt_key = 'business_guidance_prompt' if not current_product_type else 'task_management_prompt'
    print(f"Main Agent using prompt: '{prompt_key}'")

    prompt_template = ALL_PROMPTS.get('main_agent', {}).get(prompt_key, '')
    if not prompt_template:
        return {**state, "error_message": "Router prompt not found.", "main_agent_routing_decision": "unclear_input", "is_final_turn_response": True}

    parser = initial_task_decision_parser if not current_product_type else main_router_decision_parser
    format_instructions = parser.get_format_instructions()
    
    try:
        prompt_kwargs = {"user_input": user_input, "format_instructions": format_instructions}
        
        # business_guidance_promptÏóê ÏÑúÎπÑÏä§ ÏÑ§Î™Ö Ï∂îÍ∞Ä
        if not current_product_type:
            # service_descriptions.yaml Î°úÎìú
            service_desc_path = Path(__file__).parent.parent / "config" / "service_descriptions.yaml"
            if service_desc_path.exists():
                with open(service_desc_path, 'r', encoding='utf-8') as f:
                    service_data = yaml.safe_load(f)
                    
                # ÏÑúÎπÑÏä§ ÏÑ§Î™Ö Ìè¨Îß∑ÌåÖ
                service_descriptions = ""
                for service_id in ["didimdol", "jeonse", "deposit_account"]:
                    if service_id in service_data:
                        svc = service_data[service_id]
                        service_descriptions += f"\n**{svc['name']}** ({service_id})\n"
                        service_descriptions += f"- ÎåÄÏÉÅ: {svc['target']}\n"
                        service_descriptions += f"- ÏÑ§Î™Ö: {svc['summary'].strip()}\n"
                        if 'benefits' in svc:
                            service_descriptions += f"- Ï£ºÏöî ÌòúÌÉù: {', '.join(svc['benefits'][:2])}\n"
                
                prompt_kwargs["service_descriptions"] = service_descriptions
            else:
                # Ìè¥Î∞±: Í∏∞Î≥∏ ÏÑ§Î™Ö ÏÇ¨Ïö©
                prompt_kwargs["service_descriptions"] = """
**ÎîîÎî§Îèå ÎåÄÏ∂ú** (didimdol)
- ÎåÄÏÉÅ: Î¨¥Ï£ºÌÉù ÏÑúÎØº (Ïó∞ÏÜåÎìù 6-7Ï≤úÎßåÏõê Ïù¥Ìïò)
- ÏÑ§Î™Ö: Ï†ïÎ∂Ä ÏßÄÏõê Ï£ºÌÉùÍµ¨ÏûÖÏûêÍ∏à ÎåÄÏ∂ú, ÏµúÎåÄ 3-4ÏñµÏõê, Ïó∞ 2.15~2.75%

**Ï†ÑÏÑ∏ ÎåÄÏ∂ú** (jeonse)  
- ÎåÄÏÉÅ: Î¨¥Ï£ºÌÉù ÏÑ∏ÎåÄÏ£º
- ÏÑ§Î™Ö: Ï†ÑÏÑ∏ Î≥¥Ï¶ùÍ∏à ÎåÄÏ∂ú, Î≥¥Ï¶ùÍ∏àÏùò 80-90%, ÎßåÍ∏∞ÏùºÏãúÏÉÅÌôò

**ÏûÖÏ∂úÍ∏àÌÜµÏû•** (deposit_account)
- ÎåÄÏÉÅ: Î™®Îì† Í≥†Í∞ù
- ÏÑ§Î™Ö: Í∏∞Î≥∏ Í≥ÑÏ¢å, ÌèâÏÉùÍ≥ÑÏ¢å ÏÑúÎπÑÏä§, Ï≤¥ÌÅ¨Ïπ¥Îìú/Ïù∏ÌÑ∞ÎÑ∑Î±ÖÌÇπ ÎèôÏãú Ïã†Ï≤≠
"""
        
        if current_product_type:
             active_scenario_data = get_active_scenario_data(state) or {}
             current_stage_id = state.get("current_scenario_stage_id", "N/A")
             current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})
             valid_choices = current_stage_info.get("choices", []) 
             available_types = ", ".join([ALL_SCENARIOS_DATA[pt]["scenario_name"] for pt in state.get("available_product_types", []) if pt in ALL_SCENARIOS_DATA])
             
             # ÏóÖÎ¨¥ Í¥ÄÎ†® JSON Ï†ïÎ≥¥ Ï∂îÍ∞Ä
             task_context = {
                 "collected_info": state.get("collected_product_info", {}),
                 "current_stage": current_stage_info,
                 "stage_id": current_stage_id,
                 "expected_info": current_stage_info.get("expected_info_key", ""),
                 "valid_choices": valid_choices
             }
             
             # Îß§Îâ¥Ïñº Ï†ïÎ≥¥ Î°úÎìú
             product_type = state.get("current_product_type")
             manual_content = await load_knowledge_base_content_async(product_type) if product_type else ""
             
             prompt_kwargs.update({
                "active_scenario_name": state.get("active_scenario_name", "Not Selected"),
                "formatted_messages_history": format_messages_for_prompt(state.get("messages", [])[:-1]),
                "task_context_json": json.dumps(task_context, ensure_ascii=False, indent=2),
                "manual_content": manual_content[:2000] if manual_content else "Îß§Îâ¥Ïñº Ï†ïÎ≥¥ ÏóÜÏùå",
                "available_product_types_display": available_types
             })
        else:
            # Ï¥àÍ∏∞ ÌîÑÎ°¨ÌîÑÌä∏Ïóê ÌïÑÏöîÌïú available_product_types_listÎ•º Ï∂îÍ∞ÄÌï©ÎãàÎã§.
            available_types_list = state.get("available_product_types", [])
            available_services = {
                "didimdol": "ÎîîÎî§Îèå ÎåÄÏ∂ú - Ï£ºÌÉùÍµ¨ÏûÖÏùÑ ÏúÑÌïú Ï†ïÎ∂ÄÏßÄÏõê ÎåÄÏ∂ú",
                "jeonse": "Ï†ÑÏÑ∏ÏûêÍ∏àÎåÄÏ∂ú - Ï†ÑÏÑ∏ Î≥¥Ï¶ùÍ∏à ÎßàÎ†®ÏùÑ ÏúÑÌïú ÎåÄÏ∂ú", 
                "deposit_account": "ÏûÖÏ∂úÍ∏àÌÜµÏû• - ÏùºÏÉÅÏ†ÅÏù∏ Í∏àÏúµÍ±∞ÎûòÎ•º ÏúÑÌïú Í∏∞Î≥∏ Í≥ÑÏ¢å"
            }
            
            service_descriptions = [f"- {available_services.get(pt, pt)}" for pt in available_types_list]
            
            prompt_kwargs.update({
                "available_product_types_list": available_types_list,
                "available_services_description": "\n".join(service_descriptions)
            })
        
        prompt_filled = prompt_template.format(**prompt_kwargs)
        response = await json_llm.ainvoke([HumanMessage(content=prompt_filled)])
        raw_content = response.content.strip().replace("```json", "").replace("```", "").strip()
        decision = parser.parse(raw_content)

        # ÏÉàÎ°úÏö¥ ActionModel Íµ¨Ï°∞Î•º ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
        action_plan_models = decision.actions
        action_plan_tools = [action.tool for action in action_plan_models]

        new_state = {}
        if hasattr(decision, 'direct_response') and decision.direct_response:
            new_state["main_agent_direct_response"] = decision.direct_response

        system_log = f"Main Agent Plan: actions={[f'{a.tool}({a.tool_input})' for a in action_plan_models]}"
        updated_messages = list(state.get("messages", [])) + [SystemMessage(content=system_log)]
        new_state["messages"] = updated_messages

        # Ï¥àÍ∏∞ ÏÉÅÌÉú Î∂ÑÍ∏∞ Ï≤òÎ¶¨: action_plan_models ÏûêÏ≤¥Î•º ÏàòÏ†ïÌïòÏó¨ ÏùºÍ¥ÄÏÑ± Ïú†ÏßÄ
        if not current_product_type:
            first_action = action_plan_models[0] if action_plan_models else None
            if first_action:
                if first_action.tool == "set_product_type":
                    new_state["loan_selection_is_fresh"] = True
                elif first_action.tool == "invoke_qa_agent_general":
                    # action_plan_modelsÏùò tool Ïù¥Î¶ÑÏùÑ ÏßÅÏ†ë Î≥ÄÍ≤Ω
                    first_action.tool = "invoke_qa_agent"
                    new_state["active_scenario_name"] = "General Financial Advice"
                elif first_action.tool == "clarify_product_type":
                    # action_plan_modelsÏùò tool Ïù¥Î¶ÑÏùÑ ÏßÅÏ†ë Î≥ÄÍ≤Ω
                    first_action.tool = "select_product_type"

        # ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í≤∞Ï†ïÎêú Î™®Îç∏ÏóêÏÑú action_planÍ≥º action_plan_structÎ•º ÏÉùÏÑ±
        new_state["action_plan"] = [model.tool for model in action_plan_models]
        new_state["action_plan_struct"] = [model.model_dump() for model in action_plan_models]

        action_plan = new_state.get('action_plan', [])
        direct_resp = new_state.get('main_agent_direct_response', '')
        if direct_resp:
            log_node_execution("Orchestrator", output_info=f"direct_response='{direct_resp[:30]}...'")
        else:
            log_node_execution("Orchestrator", output_info=f"plan={action_plan}")
        return new_state

    except Exception as e:
        print(f"Main Agent Orchestrator Error: {e}"); traceback.print_exc()
        err_msg = "Error processing request. Please try again."
        return {**state, "error_message": err_msg, "main_agent_routing_decision": "unclear_input", "is_final_turn_response": True}

async def factual_answer_node(state: AgentState) -> dict:
    original_question = state.get("stt_result", "")
    log_node_execution("RAG_Worker", f"query='{original_question[:30]}...'")
    original_question = state.get("stt_result", "")
    messages = state.get("messages", [])
    chat_history = format_messages_for_prompt(messages[:-1]) if len(messages) > 1 else "No previous conversation."
    scenario_name = state.get("active_scenario_name", "General Financial Advice")

    if not rag_service.is_ready():
        print("Warning: RAG service is not ready. Using fallback response.")
        return {"factual_response": "Ï£ÑÏÜ°Ìï©ÎãàÎã§, ÌòÑÏû¨ Ï†ïÎ≥¥ Í≤ÄÏÉâ Í∏∞Îä•Ïóê Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïòÏó¨ ÎãµÎ≥ÄÏùÑ ÎìúÎ¶¥ Ïàò ÏóÜÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî."}

    all_queries = [original_question]
    try:
        # 1. ÏßàÎ¨∏ ÌôïÏû•
        print("--- Generating expanded queries... ---")
        expansion_prompt_template = ALL_PROMPTS.get('qa_agent', {}).get('rag_query_expansion_prompt')
        if not expansion_prompt_template:
            raise ValueError("RAG query expansion prompt not found.")
        
        expansion_prompt = ChatPromptTemplate.from_template(expansion_prompt_template)
        
        expansion_chain = expansion_prompt | json_llm | expanded_queries_parser
        expanded_result = await expansion_chain.ainvoke({
            "scenario_name": scenario_name,
            "chat_history": chat_history,
            "user_question": original_question
        })
        
        if expanded_result and expanded_result.queries:
            all_queries.extend(expanded_result.queries)
            print(f"Expanded queries generated: {expanded_result.queries}")
        else:
            print("Query expansion did not produce results. Using original question only.")

    except Exception as e:
        # ÏßàÎ¨∏ ÌôïÏû•Ïóê Ïã§Ìå®ÌïòÎçîÎùºÎèÑ, ÏõêÎ≥∏ ÏßàÎ¨∏ÏúºÎ°ú Í≥ÑÏÜç ÏßÑÌñâ
        print(f"Could not expand query due to an error: {e}. Proceeding with original question.")

    try:
        # 2. RAG ÌååÏù¥ÌîÑÎùºÏù∏ Ìò∏Ï∂ú (ÏõêÎ≥∏ + ÌôïÏû• ÏßàÎ¨∏)
        print(f"Invoking RAG pipeline with {len(all_queries)} queries.")
        factual_response = await rag_service.answer_question(all_queries, original_question)
        print(f"RAG response: {factual_response[:100]}...")
    except Exception as e:
        print(f"Factual Answer Node Error (RAG): {e}")
        factual_response = "Ï†ïÎ≥¥Î•º Í≤ÄÏÉâÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."

    # Îã§Ïùå Ïï°ÏÖòÏùÑ ÏúÑÌï¥ planÍ≥º structÏóêÏÑú ÌòÑÏû¨ Ïï°ÏÖò Ï†úÍ±∞
    updated_plan = state.get("action_plan", []).copy()
    if updated_plan:
        updated_plan.pop(0)
    
    updated_struct = state.get("action_plan_struct", []).copy()
    if updated_struct:
        updated_struct.pop(0)

    log_node_execution("RAG_Worker", output_info=f"response='{factual_response[:40]}...'")
    return {"factual_response": factual_response, "action_plan": updated_plan, "action_plan_struct": updated_struct}

async def web_search_node(state: AgentState) -> dict:
    """
    Web Search Worker - Ïô∏Î∂Ä Ï†ïÎ≥¥ Í≤ÄÏÉâ Ï†ÑÎ¨∏ Ï≤òÎ¶¨
    """
    action_struct = state.get("action_plan_struct", [{}])[0]
    query = action_struct.get("tool_input", {}).get("query", "")
    log_node_execution("Web_Worker", f"query='{query[:30]}...'")
    action_struct = state.get("action_plan_struct", [{}])[0]
    query = action_struct.get("tool_input", {}).get("query", "")
    
    if not query:
        return {"factual_response": "Î¨¥ÏóáÏóê ÎåÄÌï¥ Í≤ÄÏÉâÌï†ÏßÄ ÏïåÎ†§Ï£ºÏÑ∏Ïöî."}

    # 1. Perform web search
    search_results = await web_search_service.asearch(query)
    
    # 2. Synthesize a natural language answer from the results
    try:
        synthesis_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful AI assistant. Your task is to synthesize the provided web search results into a concise and natural-sounding answer to the user's question. Respond in Korean."),
            ("human", "User Question: {query}\n\nWeb Search Results:\n---\n{search_results}\n---\n\nSynthesized Answer:")
        ])
        
        synthesis_chain = synthesis_prompt | generative_llm
        response = await synthesis_chain.ainvoke({"query": query, "search_results": search_results})
        final_answer = response.content.strip()
        print(f"Synthesized web search answer: {final_answer[:100]}...")

    except Exception as e:
        print(f"Error synthesizing web search results: {e}")
        final_answer = "Ïõπ Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º ÏöîÏïΩÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÏõêÎ≥∏ Í≤ÄÏÉâ Í≤∞Í≥ºÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n\n" + search_results

    # Îã§Ïùå Ïï°ÏÖòÏùÑ ÏúÑÌï¥ planÍ≥º structÏóêÏÑú ÌòÑÏû¨ Ïï°ÏÖò Ï†úÍ±∞
    updated_plan = state.get("action_plan", []).copy()
    if updated_plan:
        updated_plan.pop(0)
    
    updated_struct = state.get("action_plan_struct", []).copy()
    if updated_struct:
        updated_struct.pop(0)
        
    # Ïõπ Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º ÏÇ¨Ïã§ Í∏∞Î∞ò ÎãµÎ≥ÄÏúºÎ°ú Í∞ÑÏ£ºÌïòÏó¨ factual_responseÏóê Ï†ÄÏû•
    log_node_execution("Web_Worker", output_info=f"response='{final_answer[:40]}...'")
    return {"factual_response": final_answer, "action_plan": updated_plan, "action_plan_struct": updated_struct}

async def call_scenario_agent_node(state: AgentState) -> AgentState:
    user_input = state.get("stt_result", "")
    scenario_name = state.get("active_scenario_name", "N/A")
    log_node_execution("Scenario_NLU", f"scenario={scenario_name}, input='{user_input[:20]}...'")
    user_input = state.get("stt_result", "")
    active_scenario_data = get_active_scenario_data(state)
    if not active_scenario_data or not user_input:
        return {**state, "scenario_agent_output": cast(ScenarioAgentOutput, {"intent": "error_missing_data", "is_scenario_related": False})}
    
    current_stage_id = state.get("current_scenario_stage_id", active_scenario_data.get("initial_stage_id"))
    current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})

    output = await invoke_scenario_agent_logic(
        user_input=user_input,
        current_stage_prompt=current_stage_info.get("prompt", ""),
        expected_info_key=current_stage_info.get("expected_info_key"),
        messages_history=state.get("messages", [])[:-1],
        scenario_name=active_scenario_data.get("scenario_name", "Consultation")
    )
    intent = output.get("intent", "N/A")
    
    entities = list(output.get("entities", {}).keys())
    log_node_execution("Scenario_NLU", output_info=f"intent={intent}, entities={entities}")
    return {**state, "scenario_agent_output": output}

async def process_scenario_logic_node(state: AgentState) -> AgentState:
    current_stage_id = state.get("current_scenario_stage_id", "N/A")
    scenario_name = state.get("active_scenario_name", "N/A")
    log_node_execution("Scenario_Flow", f"scenario={scenario_name}, stage={current_stage_id}")
    active_scenario_data = get_active_scenario_data(state)
    current_stage_id = state.get("current_scenario_stage_id")
    
    # Ïä§ÌÖåÏù¥ÏßÄ IDÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Ï¥àÍ∏∞ Ïä§ÌÖåÏù¥ÏßÄÎ°ú ÏÑ§Ï†ï
    if not current_stage_id:
        current_stage_id = active_scenario_data.get("initial_stage_id", "greeting")
        print(f"Ïä§ÌÖåÏù¥ÏßÄ IDÍ∞Ä ÏóÜÏñ¥ÏÑú Ï¥àÍ∏∞ Ïä§ÌÖåÏù¥ÏßÄÎ°ú ÏÑ§Ï†ï: {current_stage_id}")
    
    current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})
    print(f"ÌòÑÏû¨ Ïä§ÌÖåÏù¥ÏßÄ: {current_stage_id}, Ïä§ÌÖåÏù¥ÏßÄ Ï†ïÎ≥¥: {current_stage_info.keys()}")
    collected_info = state.get("collected_product_info", {}).copy()
    scenario_output = state.get("scenario_agent_output")
    user_input = state.get("stt_result", "")
    
    # Í∞úÏÑ†Îêú Îã§Ï§ë Ï†ïÎ≥¥ ÏàòÏßë Ï≤òÎ¶¨
    print(f"Ïä§ÌÖåÏù¥ÏßÄ Ï†ïÎ≥¥ ÌôïÏù∏ - collect_multiple_info: {current_stage_info.get('collect_multiple_info')}")
    if current_stage_info.get("collect_multiple_info"):
        print("--- Îã§Ï§ë Ï†ïÎ≥¥ ÏàòÏßë Î™®Îìú ---")
        return await process_multiple_info_collection(state, active_scenario_data, current_stage_id, current_stage_info, collected_info, user_input)
    
    # Í∏∞Ï°¥ Îã®Ïùº Ï†ïÎ≥¥ ÏàòÏßë Ï≤òÎ¶¨
    return await process_single_info_collection(state, active_scenario_data, current_stage_id, current_stage_info, collected_info, scenario_output, user_input)

async def process_multiple_info_collection(state: AgentState, active_scenario_data: Dict, current_stage_id: str, current_stage_info: Dict, collected_info: Dict, user_input: str) -> AgentState:
    """Îã§Ï§ë Ï†ïÎ≥¥ ÏàòÏßë Ï≤òÎ¶¨ (Í∞úÏÑ†Îêú Í∑∏Î£πÎ≥Ñ Î∞©Ïãù)"""
    required_fields = active_scenario_data.get("required_info_fields", [])
    
    # ÌòÑÏû¨ Ïä§ÌÖåÏù¥ÏßÄÍ∞Ä Ï†ïÎ≥¥ ÏàòÏßë Îã®Í≥ÑÏù∏ÏßÄ ÌôïÏù∏
    print(f"ÌòÑÏû¨ Ïä§ÌÖåÏù¥ÏßÄ ID: {current_stage_id}")
    if current_stage_id in ["info_collection_guidance", "process_collected_info", "ask_missing_info_group1", "ask_missing_info_group2", "ask_missing_info_group3", "eligibility_assessment"]:
        
        # Entity AgentÎ•º ÏÇ¨Ïö©Ìïú Ï†ïÎ≥¥ Ï∂îÏ∂ú
        if user_input:
            from ..agents.entity_agent import entity_agent
            
            # Entity AgentÎ°ú Ï†ïÎ≥¥ Ï∂îÏ∂ú
            extraction_result = await entity_agent.process_slot_filling(user_input, required_fields, collected_info)
            
            # Ï∂îÏ∂úÎêú Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏
            collected_info = extraction_result["collected_info"]
            print(f"Entity Agent Ï∂îÏ∂ú Í≤∞Í≥º: {extraction_result['extracted_entities']}")
            print(f"ÏµúÏ¢Ö ÏóÖÎç∞Ïù¥Ìä∏Îêú ÏàòÏßë Ï†ïÎ≥¥: {collected_info}")
        
        # Ï†ïÎ≥¥ ÏàòÏßë ÏôÑÎ£å Ïó¨Î∂Ä ÌôïÏù∏
        is_complete, missing_field_names = check_required_info_completion(collected_info, required_fields)
        
        if current_stage_id == "info_collection_guidance":
            # Ï¥àÍ∏∞ Ï†ïÎ≥¥ ÏïàÎÇ¥ ÌõÑ Î∞îÎ°ú Îã§Ïùå Í∑∏Î£π ÏßàÎ¨∏ Í≤∞Ï†ï
            if is_complete:
                next_stage_id = "eligibility_assessment"
                response_text = "ÎÑ§, Î™®Îì† Ï†ïÎ≥¥Í∞Ä ÏàòÏßëÎêòÏóàÏäµÎãàÎã§. Ïù¥Ï†ú ÏûêÍ≤© ÏöîÍ±¥ÏùÑ ÌôïÏù∏Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§."
            else:
                # ÏàòÏßëÎêú Ï†ïÎ≥¥Ïóê Îî∞Îùº Îã§Ïùå Í∑∏Î£π ÏßàÎ¨∏ Í≤∞Ï†ï
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                if next_stage_id == "eligibility_assessment":
                    response_text = "ÎÑ§, Î™®Îì† Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌñàÏäµÎãàÎã§! ÎßêÏîÄÌï¥Ï£ºÏã† Ï°∞Í±¥ÏúºÎ°ú ÎîîÎî§Îèå ÎåÄÏ∂ú Ïã†Ï≤≠Ïù¥ Í∞ÄÎä•Ìï¥ Î≥¥ÏûÖÎãàÎã§. Ïù¥Ï†ú Ïã†Ï≤≠Ïóê ÌïÑÏöîÌïú ÏÑúÎ•òÏôÄ Ï†àÏ∞®Î•º ÏïàÎÇ¥Ìï¥ÎìúÎ¶¥Í≤åÏöî."
                else:
                    response_text = f"ÎÑ§, ÎßêÏîÄÌï¥Ï£ºÏã† Ï†ïÎ≥¥ ÌôïÏù∏ÌñàÏäµÎãàÎã§! {generate_group_specific_prompt(next_stage_id, collected_info)}"
                print(f"info_collection_guidance -> {next_stage_id}, ÏùëÎãµ: {response_text}")
                
        elif current_stage_id == "process_collected_info":
            # ÏàòÏßëÎêú Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Îã§Ïùå Í∑∏Î£π Í≤∞Ï†ï
            if is_complete:
                next_stage_id = "eligibility_assessment"
                response_text = "ÎÑ§, Î™®Îì† Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌñàÏäµÎãàÎã§! ÎßêÏîÄÌï¥Ï£ºÏã† Ï°∞Í±¥ÏúºÎ°ú ÎîîÎî§Îèå ÎåÄÏ∂ú Ïã†Ï≤≠Ïù¥ Í∞ÄÎä•Ìï¥ Î≥¥ÏûÖÎãàÎã§. Ïù¥Ï†ú Ïã†Ï≤≠Ïóê ÌïÑÏöîÌïú ÏÑúÎ•òÏôÄ Ï†àÏ∞®Î•º ÏïàÎÇ¥Ìï¥ÎìúÎ¶¥Í≤åÏöî."
            else:
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                response_text = generate_group_specific_prompt(next_stage_id, collected_info)
                print(f"Îã§Ïùå Îã®Í≥ÑÎ°ú Ïù¥Îèô: {next_stage_id}, ÏßàÎ¨∏: {response_text}")
                
        elif current_stage_id.startswith("ask_missing_info_group"):
            # Í∑∏Î£πÎ≥Ñ ÏßàÎ¨∏ Ï≤òÎ¶¨ ÌõÑ Îã§Ïùå Îã®Í≥Ñ Í≤∞Ï†ï
            if is_complete:
                next_stage_id = "eligibility_assessment"
                response_text = "ÎÑ§, Î™®Îì† Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌñàÏäµÎãàÎã§! ÎßêÏîÄÌï¥Ï£ºÏã† Ï°∞Í±¥ÏúºÎ°ú ÎîîÎî§Îèå ÎåÄÏ∂ú Ïã†Ï≤≠Ïù¥ Í∞ÄÎä•Ìï¥ Î≥¥ÏûÖÎãàÎã§. Ïù¥Ï†ú Ïã†Ï≤≠Ïóê ÌïÑÏöîÌïú ÏÑúÎ•òÏôÄ Ï†àÏ∞®Î•º ÏïàÎÇ¥Ìï¥ÎìúÎ¶¥Í≤åÏöî."
            else:
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                # Í∞ôÏùÄ Í∑∏Î£πÏù¥Î©¥ Í∑∏ÎåÄÎ°ú, Îã§Î•∏ Í∑∏Î£πÏù¥Î©¥ ÏÉàÎ°úÏö¥ ÏßàÎ¨∏
                if next_stage_id == current_stage_id:
                    # Í∞ôÏùÄ Í∑∏Î£π ÎÇ¥ÏóêÏÑú ÏïÑÏßÅ Îçî ÏàòÏßëÌï† Ï†ïÎ≥¥Í∞Ä ÏûàÎäî Í≤ΩÏö∞
                    response_text = generate_group_specific_prompt(next_stage_id, collected_info)
                else:
                    # Îã§Ïùå Í∑∏Î£πÏúºÎ°ú ÎÑòÏñ¥Í∞ÄÎäî Í≤ΩÏö∞
                    response_text = generate_group_specific_prompt(next_stage_id, collected_info)
                    
        elif current_stage_id == "eligibility_assessment":
            # ÏûêÍ≤© Í≤ÄÌÜ† ÏôÑÎ£å ÌõÑ ÏÑúÎ•ò ÏïàÎÇ¥Î°ú ÏûêÎèô ÏßÑÌñâ
            next_stage_id = "application_documents_guidance"
            response_text = active_scenario_data.get("stages", {}).get("application_documents_guidance", {}).get("prompt", "ÏÑúÎ•ò ÏïàÎÇ¥Î•º ÏßÑÌñâÌïòÍ≤†ÏäµÎãàÎã§.")
            print(f"ÏûêÍ≤© Í≤ÄÌÜ† ÏôÑÎ£å -> ÏÑúÎ•ò ÏïàÎÇ¥ Îã®Í≥ÑÎ°ú Ïù¥Îèô")
            
        else:
            next_stage_id = current_stage_info.get("default_next_stage_id", "eligibility_assessment")
            response_text = current_stage_info.get("prompt", "")
        
        # ÏùëÎãµ ÌÖçÏä§Ìä∏Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
        if "response_text" not in locals():
            response_text = current_stage_info.get("prompt", "Ï∂îÍ∞Ä Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÍ≤†Ïñ¥Ïöî?")
        
        # Îã§Ïùå Ïï°ÏÖòÏùÑ ÏúÑÌï¥ planÍ≥º structÏóêÏÑú ÌòÑÏû¨ Ïï°ÏÖò Ï†úÍ±∞ (Î¨¥Ìïú Î£®ÌîÑ Î∞©ÏßÄ)
        updated_plan = state.get("action_plan", []).copy()
        if updated_plan:
            updated_plan.pop(0)
        
        updated_struct = state.get("action_plan_struct", []).copy()
        if updated_struct:
            updated_struct.pop(0)
            
        return {
            **state, 
            "current_scenario_stage_id": next_stage_id,
            "collected_product_info": collected_info,
            "final_response_text_for_tts": response_text,
            "is_final_turn_response": True,
            "action_plan": updated_plan,
            "action_plan_struct": updated_struct
        }
    
    # ÏùºÎ∞ò Ïä§ÌÖåÏù¥ÏßÄÎäî Í∏∞Ï°¥ Î°úÏßÅÏúºÎ°ú Ï≤òÎ¶¨
    return await process_single_info_collection(state, active_scenario_data, current_stage_id, current_stage_info, collected_info, state.get("scenario_agent_output"), user_input)

async def process_single_info_collection(state: AgentState, active_scenario_data: Dict, current_stage_id: str, current_stage_info: Dict, collected_info: Dict, scenario_output: Optional[ScenarioAgentOutput], user_input: str) -> AgentState:
    """Í∏∞Ï°¥ Îã®Ïùº Ï†ïÎ≥¥ ÏàòÏßë Ï≤òÎ¶¨"""

    if scenario_output and scenario_output.get("is_scenario_related"):
        entities = scenario_output.get("entities", {})
        intent = scenario_output.get("intent", "")
        
        print(f"Single info collection - intent: {intent}, entities: {entities}")
        
        if entities and user_input:
            print(f"--- Verifying extracted entities: {entities} ---")
            verification_prompt_template = """
You are an exceptionally discerning assistant tasked with interpreting a user's intent. Your goal is to determine if the user has made a definitive choice or is simply asking a question about an option.

Here is the conversational context:
- The agent asked the user: "{agent_question}"
- The user replied: "{user_response}"
- From the user's reply, the following information was extracted: {entities}

Your task is to analyze the user's reply carefully. Has the user **committed** to the choice represented by the extracted information?

Consider these rules:
1.  **Direct questions are not commitments.** If the user asks "What is [option]?" or "Are there fees for [option]?", they have NOT committed.
2.  **Hypotheticals can be commitments.** If the user asks "If I choose [option], what happens next?", they ARE committing to that option for the sake of continuing the conversation.
3.  **Ambiguity means no commitment.** If it's unclear, err on the side of caution and decide it's not a commitment.

You MUST respond in JSON format with a single key "is_confirmed" (boolean). Example: {{"is_confirmed": true}}
"""
            verification_prompt = verification_prompt_template.format(
                agent_question=current_stage_info.get("prompt", ""),
                user_response=user_input,
                entities=str(entities)
            )
            
            try:
                response = await json_llm.ainvoke([HumanMessage(content=verification_prompt)])
                raw_content = response.content.strip().replace("```json", "").replace("```", "").strip()
                decision = json.loads(raw_content)
                is_confirmed = decision.get("is_confirmed", False)
                
                if is_confirmed:
                    print(f"--- Entity verification PASSED. Updating collected info. ---")
                    collected_info.update({k: v for k, v in entities.items() if v is not None})
                else:
                    print(f"--- Entity verification FAILED. Not updating collected info. ---")
            except Exception as e:
                print(f"Error during entity verification: {e}. Assuming not confirmed.")

        elif entities:
             collected_info.update({k: v for k, v in entities.items() if v is not None})

        print(f"Updated Info: {collected_info}")
    
    # Ïä§ÌÖåÏù¥ÏßÄ Ï†ÑÌôò Î°úÏßÅ Í≤∞Ï†ï
    transitions = current_stage_info.get("transitions", [])
    default_next = current_stage_info.get("default_next_stage_id", "None")
    
    # Case 1: Î∂ÑÍ∏∞Í∞Ä ÏóÜÎäî Í≤ΩÏö∞ (transitionsÍ∞Ä ÏóÜÍ±∞ÎÇò 1Í∞ú)
    if len(transitions) <= 1:
        # ÌïÑÏöîÌïú Ï†ïÎ≥¥Í∞Ä ÏàòÏßëÎêòÏóàÎäîÏßÄ ÌôïÏù∏
        expected_info_key = current_stage_info.get("expected_info_key")
        if expected_info_key and expected_info_key not in collected_info:
            # ÌïÑÏöîÌïú Ï†ïÎ≥¥Í∞Ä ÏïÑÏßÅ ÏàòÏßëÎêòÏßÄ ÏïäÏïòÏúºÎ©¥ ÌòÑÏû¨ Ïä§ÌÖåÏù¥ÏßÄ Ïú†ÏßÄ
            next_stage_id = current_stage_id
            print(f"--- ÏûêÎèô ÏßÑÌñâ Ï∞®Îã®: '{expected_info_key}' Ï†ïÎ≥¥ ÎØ∏ÏàòÏßë ---")
        elif len(transitions) == 1:
            # Îã®Ïùº Ï†ÑÌôò Í≤ΩÎ°úÍ∞Ä ÏûàÏúºÎ©¥ ÏûêÎèô ÏßÑÌñâ
            next_stage_id = transitions[0].get("next_stage_id", default_next)
            print(f"--- ÏûêÎèô ÏßÑÌñâ: Îã®Ïùº Í≤ΩÎ°ú '{current_stage_id}' ‚Üí '{next_stage_id}' ---")
        else:
            # transitionsÏù¥ ÏóÜÏúºÎ©¥ defaultÎ°ú ÏßÑÌñâ
            next_stage_id = default_next
            print(f"--- ÏûêÎèô ÏßÑÌñâ: Í∏∞Î≥∏ Í≤ΩÎ°ú '{current_stage_id}' ‚Üí '{next_stage_id}' ---")
    
    # Case 2: Î∂ÑÍ∏∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞ (transitionsÍ∞Ä 2Í∞ú Ïù¥ÏÉÅ) - LLM ÌåêÎã®
    else:
        print(f"--- LLM ÌåêÎã® ÌïÑÏöî: {len(transitions)}Í∞ú Î∂ÑÍ∏∞ Ï°¥Ïû¨ ---")
        prompt_template = ALL_PROMPTS.get('main_agent', {}).get('determine_next_scenario_stage', '')
        llm_prompt = prompt_template.format(
            active_scenario_name=active_scenario_data.get("scenario_name"),
            current_stage_id=str(current_stage_id),
            current_stage_prompt=current_stage_info.get("prompt", "No prompt"),
            user_input=state.get("stt_result", ""),
            scenario_agent_intent=scenario_output.get("intent", "N/A") if scenario_output else "N/A",
            scenario_agent_entities=str(scenario_output.get("entities", {}) if scenario_output else {}),
            collected_product_info=str(collected_info),
            formatted_transitions=format_transitions_for_prompt(transitions, current_stage_info.get("prompt", "")),
            default_next_stage_id=default_next
        )
        response = await json_llm.ainvoke([HumanMessage(content=llm_prompt)])
        decision_data = next_stage_decision_parser.parse(response.content)
        next_stage_id = decision_data.chosen_next_stage_id

    # --- Î°úÏßÅ Ï†ÑÏö© Ïä§ÌÖåÏù¥ÏßÄ Ï≤òÎ¶¨ Î£®ÌîÑ ---
    while True:
        if not next_stage_id or str(next_stage_id).startswith("END"):
            break  # Ï¢ÖÎ£å ÏÉÅÌÉúÏóê ÎèÑÎã¨ÌïòÎ©¥ Î£®ÌîÑ ÌÉàÏ∂ú

        next_stage_info = active_scenario_data.get("stages", {}).get(str(next_stage_id), {})
        
        # Ïä§ÌÖåÏù¥ÏßÄÏóê `prompt`Í∞Ä ÏûàÏúºÎ©¥ 'ÎßêÌïòÎäî Ïä§ÌÖåÏù¥ÏßÄ'Î°ú Í∞ÑÏ£ºÌïòÍ≥† Î£®ÌîÑ ÌÉàÏ∂ú
        if next_stage_info.get("prompt"):
            break
        
        # `prompt`Í∞Ä ÏóÜÎäî Î°úÏßÅ Ï†ÑÏö© Ïä§ÌÖåÏù¥ÏßÄÏù∏ Í≤ΩÏö∞, ÏûêÎèôÏúºÎ°ú Îã§Ïùå Îã®Í≥Ñ ÏßÑÌñâ
        print(f"--- Logic Stage Detected: '{next_stage_id}'. Resolving next step automatically. ---")
        
        current_stage_id_for_prompt = str(next_stage_id)
        
        # Î°úÏßÅ Ïä§ÌÖåÏù¥ÏßÄÏö© prompt_template Ï†ïÏùò
        prompt_template = ALL_PROMPTS.get('main_agent', {}).get('determine_next_scenario_stage', '')
        llm_prompt = prompt_template.format(
            active_scenario_name=active_scenario_data.get("scenario_name"),
            current_stage_id=current_stage_id_for_prompt,
            current_stage_prompt=next_stage_info.get("prompt", "No prompt"),
            user_input="<NO_USER_INPUT_PROCEED_AUTOMATICALLY>", # ÏÇ¨Ïö©Ïûê ÏûÖÎ†•Ïù¥ ÏóÜÏùåÏùÑ Î™ÖÏãú
            scenario_agent_intent="automatic_transition",
            scenario_agent_entities=str({}),
            collected_product_info=str(collected_info),
            formatted_transitions=format_transitions_for_prompt(next_stage_info.get("transitions", []), next_stage_info.get("prompt", "")),
            default_next_stage_id=next_stage_info.get("default_next_stage_id", "None")
        )
        response = await json_llm.ainvoke([HumanMessage(content=llm_prompt)])
        decision_data = next_stage_decision_parser.parse(response.content)
        
        next_stage_id = decision_data.chosen_next_stage_id # Îã§Ïùå Ïä§ÌÖåÏù¥ÏßÄ IDÎ•º Í∞±Ïã†ÌïòÍ≥† Î£®ÌîÑ Í≥ÑÏÜç

    # ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í≤∞Ï†ïÎêú 'ÎßêÌïòÎäî' Ïä§ÌÖåÏù¥ÏßÄ ID
    determined_next_stage_id = next_stage_id
    
    updated_plan = state.get("action_plan", []).copy()
    if updated_plan:
        updated_plan.pop(0)
    
    updated_struct = state.get("action_plan_struct", []).copy()
    if updated_struct:
        updated_struct.pop(0)

    return {
        **state, 
        "collected_product_info": collected_info, 
        "current_scenario_stage_id": determined_next_stage_id,
        "action_plan": updated_plan,
        "action_plan_struct": updated_struct
    }

async def synthesize_response_node(state: AgentState) -> dict:
    has_factual = bool(state.get("factual_response"))
    has_contextual = bool(state.get("current_product_type"))
    has_direct = bool(state.get("main_agent_direct_response"))
    log_node_execution("Synthesizer", f"factual={has_factual}, contextual={has_contextual}, direct={has_direct}")
    
    # 1. Ïù¥ÎØ∏ final_response_text_for_ttsÍ∞Ä ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏúºÎ©¥ Í∑∏Í≤ÉÏùÑ Ïö∞ÏÑ† ÏÇ¨Ïö©
    existing_response = state.get("final_response_text_for_tts")
    if existing_response:
        print(f"Ïù¥ÎØ∏ ÏÑ§Ï†ïÎêú ÏùëÎãµ ÏÇ¨Ïö©: {existing_response}")
        updated_messages = list(state['messages']) + [AIMessage(content=existing_response)]
        return {"final_response_text_for_tts": existing_response, "messages": updated_messages, "is_final_turn_response": True}
    
    # 2. main_agent_direct_responseÍ∞Ä ÏûàÏúºÎ©¥ Ïö∞ÏÑ† ÏÇ¨Ïö© (business_guidanceÏóêÏÑú ÏÉùÏÑ±Îêú ÏùëÎãµ)
    direct_response = state.get("main_agent_direct_response")
    if direct_response:
        print(f"Main agent direct response ÏÇ¨Ïö©: {direct_response[:50]}...")
        updated_messages = list(state['messages']) + [AIMessage(content=direct_response)]
        return {"final_response_text_for_tts": direct_response, "messages": updated_messages, "is_final_turn_response": True}
    
    user_question = state["messages"][-1].content
    factual_answer = state.get("factual_response", "")
    
    contextual_response = ""
    active_scenario_data = get_active_scenario_data(state)
    if active_scenario_data:
        current_stage_id = state.get("current_scenario_stage_id")
        if current_stage_id and not str(current_stage_id).startswith("END_"):
             current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})
             contextual_response = current_stage_info.get("prompt", "")
             if "%{" in contextual_response:
                import re
                if "end_scenario_message" in contextual_response:
                    contextual_response = re.sub(r'%\{end_scenario_message\}%', 
                        active_scenario_data.get("end_scenario_message", "ÏÉÅÎã¥Ïù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§. Ïù¥Ïö©Ìï¥Ï£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§."), 
                        contextual_response)
                else:
                    contextual_response = re.sub(r'%\{([^}]+)\}%', 
                        lambda m: str(state.get("collected_product_info", {}).get(m.group(1), f"")), 
                        contextual_response)
    
    if not factual_answer or "Could not find" in factual_answer:
        final_answer = contextual_response or "Ï£ÑÏÜ°Ìï©ÎãàÎã§, ÎèÑÏõÄÏùÑ ÎìúÎ¶¨ÏßÄ Î™ªÌñàÏäµÎãàÎã§."
    elif not contextual_response:
        final_answer = factual_answer
    else:
        try:
            response = await synthesizer_chain.ainvoke({
                "chat_history": state['messages'][:-1],
                "user_question": user_question,
                "contextual_response": f"After answering, you need to continue the conversation with this prompt: '{contextual_response}'",
                "factual_response": factual_answer,
            })
            final_answer = response.content.strip()
        except Exception as e:
            print(f"Synthesizer Error: {e}")
            final_answer = f"{factual_answer}\n\n{contextual_response}"

    # final_answerÍ∞Ä NoneÏù¥ ÎêòÏßÄ ÏïäÎèÑÎ°ù Î≥¥Ïû•
    if not final_answer:
        final_answer = "Ï£ÑÏÜ°Ìï©ÎãàÎã§, ÏùëÎãµÏùÑ ÏÉùÏÑ±ÌïòÎäîÎç∞ Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."

    log_node_execution("Synthesizer", output_info=f"response='{final_answer[:40]}...'")
    updated_messages = list(state['messages']) + [AIMessage(content=final_answer)]
    
    return {"final_response_text_for_tts": final_answer, "messages": updated_messages, "is_final_turn_response": True}

async def end_conversation_node(state: AgentState) -> AgentState:
    log_node_execution("End_Conversation", "terminating session")
    response_text = "ÏÉÅÎã¥ÏùÑ Ï¢ÖÎ£åÌï©ÎãàÎã§. Ïù¥Ïö©Ìï¥Ï£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§."
    
    updated_messages = list(state.get("messages", [])) + [AIMessage(content=response_text)]
    return {
        **state, 
        "final_response_text_for_tts": response_text, 
        "messages": updated_messages, 
        "is_final_turn_response": True
    }

async def set_product_type_node(state: AgentState) -> AgentState:
    action_plan_struct = state.get("action_plan_struct", [])
    if action_plan_struct:
        product_id = action_plan_struct[0].get("tool_input", {}).get("product_id", "N/A")
        log_node_execution("Set_Product", f"product={product_id}")
    else:
        log_node_execution("Set_Product", "ERROR: no action plan")
    
    action_plan_struct = state.get("action_plan_struct", [])
    if not action_plan_struct:
        err_msg = "Action plan is empty in set_product_type_node"
        print(f"ERROR: {err_msg}")
        return {**state, "error_message": err_msg, "is_final_turn_response": True}
    
    # ÌòÑÏû¨ Ïï°ÏÖòÏóê ÎßûÎäî Íµ¨Ï°∞ Ï∞æÍ∏∞
    current_action_model = ActionModel.model_validate(action_plan_struct[0])
    
    new_product_type = current_action_model.tool_input.get("product_id")
    
    if not new_product_type:
        err_msg = f"product_id not found in action: {current_action_model.dict()}"
        print(f"ERROR: {err_msg}")
        return {**state, "error_message": err_msg, "is_final_turn_response": True}

    active_scenario = ALL_SCENARIOS_DATA.get(new_product_type)
    
    if not active_scenario:
        err_msg = f"Failed to load scenario for product type: {new_product_type}"
        print(f"ERROR: {err_msg}")
        return {**state, "error_message": err_msg, "is_final_turn_response": True}
        
    print(f"Successfully loaded scenario: {active_scenario.get('scenario_name')}")

    initial_stage_id = active_scenario.get("initial_stage_id")
    response_text = active_scenario.get("stages", {}).get(str(initial_stage_id), {}).get("prompt", "How can I help?")

    print(f"Generated response text: '{response_text[:70]}...'")

    updated_messages = list(state.get("messages", [])) + [AIMessage(content=response_text)]
    
    # ÏãúÎÇòÎ¶¨Ïò§ Ïó∞ÏÜçÏÑ±ÏùÑ ÏúÑÌïú ÏÉÅÌÉú ÏÑ§Ï†ï
    print(f"üîÑ ÏãúÎÇòÎ¶¨Ïò§ Ïó∞ÏÜçÏÑ± Ï§ÄÎπÑ: {active_scenario.get('scenario_name')}")
    
    return {
        **state, "current_product_type": new_product_type, "active_scenario_data": active_scenario,
        "active_scenario_name": active_scenario.get("scenario_name"), "current_scenario_stage_id": initial_stage_id,
        "collected_product_info": {}, "final_response_text_for_tts": response_text,
        "messages": updated_messages, "is_final_turn_response": True,
        # ÏãúÎÇòÎ¶¨Ïò§ Ïó∞ÏÜçÏÑ± Í¥ÄÎ¶¨
        "scenario_ready_for_continuation": True,
        "scenario_awaiting_user_response": True
    }
    

def route_after_scenario_logic(state: AgentState) -> str:
    return "synthesize_response_node"

def execute_plan_router(state: AgentState) -> str:
    """Í∞ÑÏÜåÌôîÎêú ÎùºÏö∞ÌÑ∞ - Worker Ï§ëÏã¨ ÎùºÏö∞ÌåÖ"""
    plan = state.get("action_plan", [])
    if not plan:
        log_node_execution("Router", "plan_complete ‚Üí synthesizer")
        return "synthesize_response_node"

    next_action = plan[0] 
    target_node = None
    
    # Worker Ï§ëÏã¨ ÎùºÏö∞ÌåÖ Îßµ
    worker_routing_map = {
        "invoke_scenario_agent": "scenario_worker",
        "invoke_qa_agent": "rag_worker", 
        "invoke_web_search": "web_worker",
        "set_product_type": "set_product_type_node",
        "end_conversation": "end_conversation_node"
    }
    target_node = worker_routing_map.get(next_action, "synthesize_response_node")
    log_node_execution("Router", f"{next_action} ‚Üí {target_node.replace('_node', '').replace('_worker', '')}")
    return target_node

# --- Orchestration-Worker Graph Build ---
workflow = StateGraph(AgentState)

# Core Orchestrator
workflow.add_node("entry_point_node", entry_point_node)
workflow.add_node("main_agent_router_node", main_agent_router_node)

# Specialized Workers
workflow.add_node("scenario_worker", call_scenario_agent_node)
workflow.add_node("scenario_flow_worker", process_scenario_logic_node) 
workflow.add_node("rag_worker", factual_answer_node)
workflow.add_node("web_worker", web_search_node)

# Response & Control Nodes
workflow.add_node("synthesize_response_node", synthesize_response_node)
workflow.add_node("set_product_type_node", set_product_type_node)
workflow.add_node("end_conversation_node", end_conversation_node)

# Orchestrator Flow
workflow.set_entry_point("entry_point_node")
workflow.add_edge("entry_point_node", "main_agent_router_node")

# Orchestrator to Workers
workflow.add_conditional_edges(
    "main_agent_router_node",
    execute_plan_router,
    {
        "scenario_worker": "scenario_worker",
        "rag_worker": "rag_worker", 
        "web_worker": "web_worker",
        "synthesize_response_node": "synthesize_response_node",
        "set_product_type_node": "set_product_type_node",
        "end_conversation_node": "end_conversation_node",
    }
)

# Worker Flows
workflow.add_edge("scenario_worker", "scenario_flow_worker")
workflow.add_conditional_edges("scenario_flow_worker", execute_plan_router)
workflow.add_conditional_edges("rag_worker", execute_plan_router)
workflow.add_conditional_edges("web_worker", execute_plan_router)

workflow.add_edge("synthesize_response_node", END)
workflow.add_edge("set_product_type_node", END)
workflow.add_edge("end_conversation_node", END)

app_graph = workflow.compile()
print("--- LangGraph compiled successfully (Orchestration-Worker Architecture). ---")

async def run_agent_streaming(
    user_input_text: Optional[str] = None,
    user_input_audio_b64: Optional[str] = None,
    session_id: Optional[str] = "default_session",
    current_state_dict: Optional[Dict[str, Any]] = None
) -> AsyncGenerator[Union[Dict[str, Any], str], None]:
    
    if not OPENAI_API_KEY or not json_llm or not generative_llm:
        error_msg = "LLM service is not initialized. Please check API key."
        yield {"type": "error", "message": error_msg}
        yield {"type": "final_state", "data": {"error_message": error_msg, "is_final_turn_response": True}}
        return

    initial_state = cast(AgentState, {
        "session_id": session_id or "default_session",
        "user_input_text": user_input_text,
        "user_input_audio_b64": user_input_audio_b64,
        "messages": current_state_dict.get("messages", []) if current_state_dict else [],
        "current_product_type": current_state_dict.get("current_product_type") if current_state_dict else None,
        "current_scenario_stage_id": current_state_dict.get("current_scenario_stage_id") if current_state_dict else None,
        "collected_product_info": current_state_dict.get("collected_product_info", {}) if current_state_dict else {},
        "available_product_types": ["didimdol", "jeonse", "deposit_account"],
        "action_plan": [],
        "action_plan_struct": [],
        # ÏãúÎÇòÎ¶¨Ïò§ Ïó∞ÏÜçÏÑ± ÏÉÅÌÉú Î≥µÏõê
        "scenario_ready_for_continuation": current_state_dict.get("scenario_ready_for_continuation", False) if current_state_dict else False,
        "scenario_awaiting_user_response": current_state_dict.get("scenario_awaiting_user_response", False) if current_state_dict else False,
    })

    print(f"\nüöÄ ===== AGENT FLOW START [{session_id}] =====")
    log_node_execution("Session", f"product={initial_state['current_product_type']}, input='{user_input_text[:30]}...'")

    final_state: Optional[AgentState] = None
    streamed_text = ""

    try:
        final_state = await app_graph.ainvoke(initial_state)
        
        if final_state and final_state.get("final_response_text_for_tts"):
            text_to_stream = final_state["final_response_text_for_tts"]
            yield {"type": "stream_start"}
            for char in text_to_stream:
                yield char
                streamed_text += char
                await asyncio.sleep(0.01)
            yield {"type": "stream_end", "full_text": streamed_text}
        else:
            error_msg = final_state.get("error_message", "Failed to generate a response.")
            yield {"type": "error", "message": error_msg}
            if final_state: final_state["final_response_text_for_tts"] = error_msg

    except Exception as e:
        print(f"CRITICAL error in run_agent_streaming for session {session_id}: {e}")
        traceback.print_exc()
        error_response = "A critical system error occurred during processing."
        yield {"type": "error", "message": error_response}
        final_state = cast(AgentState, initial_state.copy())
        final_state["error_message"] = error_response
        final_state["is_final_turn_response"] = True
        final_state["messages"] = list(initial_state.get("messages", [])) + [AIMessage(content=error_response)]
    
    finally:
        if final_state:
            yield {"type": "final_state", "data": final_state}
        else:
            final_state = initial_state
            final_state["error_message"] = "Agent execution failed critically, no final state produced."
            final_state["is_final_turn_response"] = True
            yield {"type": "final_state", "data": final_state}
        print(f"üèÅ ===== AGENT FLOW END [{session_id}] =====")
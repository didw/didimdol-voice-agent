# backend/app/graph/agent.py
import json
import yaml
import asyncio
from pathlib import Path
from typing import Dict, Optional, Sequence, Literal, Any, List, Union, cast, AsyncGenerator
import traceback

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from .state import AgentState, ScenarioAgentOutput, PRODUCT_TYPES
from .state_utils import ensure_pydantic_state, ensure_dict_state
from ..core.config import OPENAI_API_KEY, LLM_MODEL_NAME
from .models import (
    next_stage_decision_parser,
    initial_task_decision_parser,
    main_router_decision_parser,
    ActionModel,
    expanded_queries_parser
)
from .utils import (
    ALL_PROMPTS,
    ALL_SCENARIOS_DATA,
    get_active_scenario_data,
    load_knowledge_base_content_async,
    format_messages_for_prompt,
    format_transitions_for_prompt,
)
from .chains import (
    json_llm,
    generative_llm,
    synthesizer_chain,
    invoke_scenario_agent_logic
)
import re
from ..services.rag_service import rag_service
from ..services.web_search_service import web_search_service

# --- Import logger ---
from .logger import log_node_execution

# --- Helper Functions for Information Collection ---

# í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ ë¡œì§ ì œê±° - Entity Agent ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´

def check_required_info_completion(collected_info: Dict, required_fields: List[Dict]) -> tuple[bool, List[str]]:
    """í•„ìˆ˜ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
    missing_fields = []
    
    for field in required_fields:
        if field["required"] and field["key"] not in collected_info:
            missing_fields.append(field["display_name"])
    
    is_complete = len(missing_fields) == 0
    return is_complete, missing_fields

def generate_missing_info_prompt(missing_fields: List[str], collected_info: Dict) -> str:
    """ë¶€ì¡±í•œ ì •ë³´ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ ìš”ì²­ ë©”ì‹œì§€ ìƒì„±"""
    if len(missing_fields) == 1:
        return f"{missing_fields[0]}ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
    elif len(missing_fields) == 2:
        return f"{missing_fields[0]}ê³¼(ì™€) {missing_fields[1]}ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
    else:
        field_list = ", ".join(missing_fields[:-1])
        return f"{field_list}, ê·¸ë¦¬ê³  {missing_fields[-1]}ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"

def get_next_missing_info_group_stage(collected_info: Dict, required_fields: List[Dict]) -> str:
    """ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì— ë¬¼ì–´ë³¼ ê·¸ë£¹ ìŠ¤í…Œì´ì§€ ê²°ì •"""
    # ê·¸ë£¹ë³„ ì •ë³´ í™•ì¸
    group1_fields = ["loan_purpose_confirmed", "marital_status"]
    group2_fields = ["has_home", "annual_income"] 
    group3_fields = ["target_home_price"]
    
    print(f"í˜„ì¬ ìˆ˜ì§‘ëœ ì •ë³´: {collected_info}")
    
    # ê° ê·¸ë£¹ì—ì„œ ëˆ„ë½ëœ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
    group1_missing = any(field not in collected_info for field in group1_fields)
    group2_missing = any(field not in collected_info for field in group2_fields)
    group3_missing = any(field not in collected_info for field in group3_fields)
    
    print(f"ê·¸ë£¹ë³„ ëˆ„ë½ ìƒíƒœ - Group1: {group1_missing}, Group2: {group2_missing}, Group3: {group3_missing}")
    
    if group1_missing:
        return "ask_missing_info_group1"
    elif group2_missing:
        return "ask_missing_info_group2"
    elif group3_missing:
        return "ask_missing_info_group3"
    else:
        return "eligibility_assessment"

def generate_group_specific_prompt(stage_id: str, collected_info: Dict) -> str:
    """ê·¸ë£¹ë³„ë¡œ ì´ë¯¸ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì œì™¸í•˜ê³  ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±"""
    print(f"ì§ˆë¬¸ ìƒì„± - stage_id: {stage_id}, collected_info: {collected_info}")
    
    if stage_id == "ask_missing_info_group1":
        missing = []
        has_loan_purpose = collected_info.get("loan_purpose_confirmed", False)
        has_marital_status = "marital_status" in collected_info
        
        if not has_loan_purpose:
            missing.append("ëŒ€ì¶œ ëª©ì (ì£¼íƒ êµ¬ì…ìš©ì¸ì§€)")
        if not has_marital_status:
            missing.append("í˜¼ì¸ ìƒíƒœ")
        
        print(f"Group1 ëˆ„ë½ ì •ë³´: {missing}")
        
        if len(missing) == 2:
            return "ëª‡ ê°€ì§€ ë” í™•ì¸í•´ë³¼ê²Œìš”. ëŒ€ì¶œ ëª©ì ê³¼ í˜¼ì¸ ìƒíƒœëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”?"
        elif "ëŒ€ì¶œ ëª©ì (ì£¼íƒ êµ¬ì…ìš©ì¸ì§€)" in missing:
            return "ëŒ€ì¶œ ëª©ì ì„ í™•ì¸í•´ë³¼ê²Œìš”. ì£¼íƒ êµ¬ì… ëª©ì ì´ ë§ìœ¼ì‹ ê°€ìš”?"
        elif "í˜¼ì¸ ìƒíƒœ" in missing:
            return "í˜¼ì¸ ìƒíƒœëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”? (ë¯¸í˜¼/ê¸°í˜¼/ì˜ˆë¹„ë¶€ë¶€)"
        else:
            # Group1ì˜ ëª¨ë“  ì •ë³´ê°€ ìˆ˜ì§‘ëœ ê²½ìš° Group2ë¡œ ë„˜ì–´ê°€ì•¼ í•¨
            return "ì¶”ê°€ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
            
    elif stage_id == "ask_missing_info_group2":
        missing = []
        if "has_home" not in collected_info:
            missing.append("ì£¼íƒ ì†Œìœ  ì—¬ë¶€")
        if "annual_income" not in collected_info:
            missing.append("ì—°ì†Œë“")
            
        if len(missing) == 2:
            return "í˜„ì¬ ì£¼íƒ ì†Œìœ  ì—¬ë¶€ì™€ ì—°ì†Œë“ì€ ì–´ëŠ ì •ë„ ë˜ì‹œë‚˜ìš”?"
        elif "ì£¼íƒ ì†Œìœ  ì—¬ë¶€" in missing:
            return "í˜„ì¬ ì†Œìœ í•˜ê³  ê³„ì‹  ì£¼íƒì´ ìˆìœ¼ì‹ ê°€ìš”?"
        else:
            return "ì—°ì†Œë“ì€ ì–´ëŠ ì •ë„ ë˜ì‹œë‚˜ìš”? (ì„¸ì „ ê¸°ì¤€)"
            
    elif stage_id == "ask_missing_info_group3":
        return "êµ¬ë§¤ ì˜ˆì •ì´ì‹  ì£¼íƒ ê°€ê²©ì€ ì–´ëŠ ì •ë„ë¡œ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”?"
    
    return "ì¶”ê°€ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"

# --- Import Node Functions ---
from .nodes.orchestrator.entry_point import entry_point_node
from .nodes.orchestrator.main_router import main_agent_router_node
from .nodes.control.end_conversation import end_conversation_node
from .nodes.control.synthesize import synthesize_response_node
from .nodes.control.set_product import set_product_type_node
from .nodes.workers.rag_worker import factual_answer_node
from .nodes.workers.web_worker import web_search_node

# --- Import Router ---
from .router import execute_plan_router, route_after_scenario_logic

# --- LangGraph Node Functions ---

async def call_scenario_agent_node(state: AgentState) -> AgentState:
    """
    ì‹œë‚˜ë¦¬ì˜¤ ì—ì´ì „íŠ¸ í˜¸ì¶œ ë…¸ë“œ - Pydantic ë²„ì „
    """
    # Convert to Pydantic for internal processing
    pydantic_state = ensure_pydantic_state(state)
    
    user_input = pydantic_state.stt_result or ""
    scenario_name = pydantic_state.active_scenario_name or "N/A"
    # user_inputì´ Noneì¸ ê²½ìš° ì²˜ë¦¬
    input_preview = user_input[:20] if user_input else ""
    log_node_execution("Scenario_NLU", f"scenario={scenario_name}, input='{input_preview}...'")
    active_scenario_data = get_active_scenario_data(pydantic_state.to_dict())
    if not active_scenario_data or not user_input:
        state_updates = {"scenario_agent_output": cast(ScenarioAgentOutput, {"intent": "error_missing_data", "is_scenario_related": False})}
        return ensure_dict_state(pydantic_state.merge_update(state_updates))
    
    current_stage_id = pydantic_state.current_scenario_stage_id or active_scenario_data.get("initial_stage_id")
    current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})

    output = await invoke_scenario_agent_logic(
        user_input=user_input,
        current_stage_prompt=current_stage_info.get("prompt", ""),
        expected_info_key=current_stage_info.get("expected_info_key"),
        messages_history=list(pydantic_state.messages)[:-1],
        scenario_name=active_scenario_data.get("scenario_name", "Consultation")
    )
    intent = output.get("intent", "N/A")
    
    entities = list(output.get("entities", {}).keys())
    log_node_execution("Scenario_NLU", output_info=f"intent={intent}, entities={entities}")
    
    state_updates = {"scenario_agent_output": output}
    return ensure_dict_state(pydantic_state.merge_update(state_updates))

async def process_scenario_logic_node(state: AgentState) -> AgentState:
    """
    ì‹œë‚˜ë¦¬ì˜¤ ë¡œì§ ì²˜ë¦¬ ë…¸ë“œ - Pydantic ë²„ì „
    """
    # Convert to Pydantic for internal processing
    pydantic_state = ensure_pydantic_state(state)
    
    current_stage_id = pydantic_state.current_scenario_stage_id or "N/A"
    scenario_name = pydantic_state.active_scenario_name or "N/A"
    log_node_execution("Scenario_Flow", f"scenario={scenario_name}, stage={current_stage_id}")
    active_scenario_data = get_active_scenario_data(pydantic_state.to_dict())
    current_stage_id = pydantic_state.current_scenario_stage_id
    
    # ìŠ¤í…Œì´ì§€ IDê°€ ì—†ëŠ” ê²½ìš° ì´ˆê¸° ìŠ¤í…Œì´ì§€ë¡œ ì„¤ì •
    if not current_stage_id:
        current_stage_id = active_scenario_data.get("initial_stage_id", "greeting")
        print(f"ìŠ¤í…Œì´ì§€ IDê°€ ì—†ì–´ì„œ ì´ˆê¸° ìŠ¤í…Œì´ì§€ë¡œ ì„¤ì •: {current_stage_id}")
    
    current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})
    print(f"í˜„ì¬ ìŠ¤í…Œì´ì§€: {current_stage_id}, ìŠ¤í…Œì´ì§€ ì •ë³´: {current_stage_info.keys()}")
    collected_info = pydantic_state.collected_product_info.copy()
    scenario_output = pydantic_state.scenario_agent_output
    user_input = pydantic_state.stt_result or ""
    
    # ê°œì„ ëœ ë‹¤ì¤‘ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬
    print(f"ìŠ¤í…Œì´ì§€ ì •ë³´ í™•ì¸ - collect_multiple_info: {current_stage_info.get('collect_multiple_info')}")
    if current_stage_info.get("collect_multiple_info"):
        print("--- ë‹¤ì¤‘ ì •ë³´ ìˆ˜ì§‘ ëª¨ë“œ ---")
        result = await process_multiple_info_collection(pydantic_state.to_dict(), active_scenario_data, current_stage_id, current_stage_info, collected_info, user_input)
        return result
    
    # ê¸°ì¡´ ë‹¨ì¼ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬
    result = await process_single_info_collection(pydantic_state.to_dict(), active_scenario_data, current_stage_id, current_stage_info, collected_info, scenario_output, user_input)
    return result

async def process_multiple_info_collection(state: AgentState, active_scenario_data: Dict, current_stage_id: str, current_stage_info: Dict, collected_info: Dict, user_input: str) -> AgentState:
    """ë‹¤ì¤‘ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬ (ê°œì„ ëœ ê·¸ë£¹ë³„ ë°©ì‹)"""
    required_fields = active_scenario_data.get("required_info_fields", [])
    
    # í˜„ì¬ ìŠ¤í…Œì´ì§€ê°€ ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ì¸ì§€ í™•ì¸
    print(f"í˜„ì¬ ìŠ¤í…Œì´ì§€ ID: {current_stage_id}")
    if current_stage_id in ["info_collection_guidance", "process_collected_info", "ask_missing_info_group1", "ask_missing_info_group2", "ask_missing_info_group3", "eligibility_assessment"]:
        
        # Entity Agentë¥¼ ì‚¬ìš©í•œ ì •ë³´ ì¶”ì¶œ
        if user_input:
            from ..agents.entity_agent import entity_agent
            
            # Entity Agentë¡œ ì •ë³´ ì¶”ì¶œ
            extraction_result = await entity_agent.process_slot_filling(user_input, required_fields, collected_info)
            
            # ì¶”ì¶œëœ ì •ë³´ ì—…ë°ì´íŠ¸
            collected_info = extraction_result["collected_info"]
            print(f"Entity Agent ì¶”ì¶œ ê²°ê³¼: {extraction_result['extracted_entities']}")
            print(f"ìµœì¢… ì—…ë°ì´íŠ¸ëœ ìˆ˜ì§‘ ì •ë³´: {collected_info}")
        
        # ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
        is_complete, missing_field_names = check_required_info_completion(collected_info, required_fields)
        
        if current_stage_id == "info_collection_guidance":
            # ì´ˆê¸° ì •ë³´ ì•ˆë‚´ í›„ ë°”ë¡œ ë‹¤ìŒ ê·¸ë£¹ ì§ˆë¬¸ ê²°ì •
            if is_complete:
                next_stage_id = "eligibility_assessment"
                response_text = "ë„¤, ëª¨ë“  ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ìê²© ìš”ê±´ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
            else:
                # ìˆ˜ì§‘ëœ ì •ë³´ì— ë”°ë¼ ë‹¤ìŒ ê·¸ë£¹ ì§ˆë¬¸ ê²°ì •
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                if next_stage_id == "eligibility_assessment":
                    response_text = "ë„¤, ëª¨ë“  ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤! ë§ì”€í•´ì£¼ì‹  ì¡°ê±´ìœ¼ë¡œ ë””ë”¤ëŒ ëŒ€ì¶œ ì‹ ì²­ì´ ê°€ëŠ¥í•´ ë³´ì…ë‹ˆë‹¤. ì´ì œ ì‹ ì²­ì— í•„ìš”í•œ ì„œë¥˜ì™€ ì ˆì°¨ë¥¼ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”."
                else:
                    response_text = f"ë„¤, ë§ì”€í•´ì£¼ì‹  ì •ë³´ í™•ì¸í–ˆìŠµë‹ˆë‹¤! {generate_group_specific_prompt(next_stage_id, collected_info)}"
                print(f"info_collection_guidance -> {next_stage_id}, ì‘ë‹µ: {response_text}")
                
        elif current_stage_id == "process_collected_info":
            # ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê·¸ë£¹ ê²°ì •
            if is_complete:
                next_stage_id = "eligibility_assessment"
                response_text = "ë„¤, ëª¨ë“  ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤! ë§ì”€í•´ì£¼ì‹  ì¡°ê±´ìœ¼ë¡œ ë””ë”¤ëŒ ëŒ€ì¶œ ì‹ ì²­ì´ ê°€ëŠ¥í•´ ë³´ì…ë‹ˆë‹¤. ì´ì œ ì‹ ì²­ì— í•„ìš”í•œ ì„œë¥˜ì™€ ì ˆì°¨ë¥¼ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”."
            else:
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                response_text = generate_group_specific_prompt(next_stage_id, collected_info)
                print(f"ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™: {next_stage_id}, ì§ˆë¬¸: {response_text}")
                
        elif current_stage_id.startswith("ask_missing_info_group"):
            # ê·¸ë£¹ë³„ ì§ˆë¬¸ ì²˜ë¦¬ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
            if is_complete:
                next_stage_id = "eligibility_assessment"
                response_text = "ë„¤, ëª¨ë“  ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤! ë§ì”€í•´ì£¼ì‹  ì¡°ê±´ìœ¼ë¡œ ë””ë”¤ëŒ ëŒ€ì¶œ ì‹ ì²­ì´ ê°€ëŠ¥í•´ ë³´ì…ë‹ˆë‹¤. ì´ì œ ì‹ ì²­ì— í•„ìš”í•œ ì„œë¥˜ì™€ ì ˆì°¨ë¥¼ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”."
            else:
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                # ê°™ì€ ê·¸ë£¹ì´ë©´ ê·¸ëŒ€ë¡œ, ë‹¤ë¥¸ ê·¸ë£¹ì´ë©´ ìƒˆë¡œìš´ ì§ˆë¬¸
                if next_stage_id == current_stage_id:
                    # ê°™ì€ ê·¸ë£¹ ë‚´ì—ì„œ ì•„ì§ ë” ìˆ˜ì§‘í•  ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
                    response_text = generate_group_specific_prompt(next_stage_id, collected_info)
                else:
                    # ë‹¤ìŒ ê·¸ë£¹ìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ê²½ìš°
                    response_text = generate_group_specific_prompt(next_stage_id, collected_info)
                    
        elif current_stage_id == "eligibility_assessment":
            # ìê²© ê²€í†  ì™„ë£Œ í›„ ì„œë¥˜ ì•ˆë‚´ë¡œ ìë™ ì§„í–‰
            next_stage_id = "application_documents_guidance"
            response_text = active_scenario_data.get("stages", {}).get("application_documents_guidance", {}).get("prompt", "ì„œë¥˜ ì•ˆë‚´ë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.")
            print(f"ìê²© ê²€í†  ì™„ë£Œ -> ì„œë¥˜ ì•ˆë‚´ ë‹¨ê³„ë¡œ ì´ë™")
            
        else:
            next_stage_id = current_stage_info.get("default_next_stage_id", "eligibility_assessment")
            response_text = current_stage_info.get("prompt", "")
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if "response_text" not in locals():
            response_text = current_stage_info.get("prompt", "ì¶”ê°€ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?")
        
        # ë‹¤ìŒ ì•¡ì…˜ì„ ìœ„í•´ planê³¼ structì—ì„œ í˜„ì¬ ì•¡ì…˜ ì œê±° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        updated_plan = state.get("action_plan", []).copy()
        if updated_plan:
            updated_plan.pop(0)
        
        updated_struct = state.get("action_plan_struct", []).copy()
        if updated_struct:
            updated_struct.pop(0)
            
        return {
            **state, 
            "current_scenario_stage_id": next_stage_id,
            "collected_product_info": collected_info,
            "final_response_text_for_tts": response_text,
            "is_final_turn_response": True,
            "action_plan": updated_plan,
            "action_plan_struct": updated_struct
        }
    
    # ì¼ë°˜ ìŠ¤í…Œì´ì§€ëŠ” ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬
    return await process_single_info_collection(state, active_scenario_data, current_stage_id, current_stage_info, collected_info, state.get("scenario_agent_output"), user_input)

async def process_single_info_collection(state: AgentState, active_scenario_data: Dict, current_stage_id: str, current_stage_info: Dict, collected_info: Dict, scenario_output: Optional[ScenarioAgentOutput], user_input: str) -> AgentState:
    """ê¸°ì¡´ ë‹¨ì¼ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬"""

    if scenario_output and scenario_output.get("is_scenario_related"):
        entities = scenario_output.get("entities", {})
        intent = scenario_output.get("intent", "")
        
        print(f"Single info collection - intent: {intent}, entities: {entities}")
        
        if entities and user_input:
            print(f"--- Verifying extracted entities: {entities} ---")
            verification_prompt_template = """
You are an exceptionally discerning assistant tasked with interpreting a user's intent. Your goal is to determine if the user has made a definitive choice or is simply asking a question about an option.

Here is the conversational context:
- The agent asked the user: "{agent_question}"
- The user replied: "{user_response}"
- From the user's reply, the following information was extracted: {entities}

Your task is to analyze the user's reply carefully. Has the user **committed** to the choice represented by the extracted information?

Consider these rules:
1.  **Direct questions are not commitments.** If the user asks "What is [option]?" or "Are there fees for [option]?", they have NOT committed.
2.  **Hypotheticals can be commitments.** If the user asks "If I choose [option], what happens next?", they ARE committing to that option for the sake of continuing the conversation.
3.  **Ambiguity means no commitment.** If it's unclear, err on the side of caution and decide it's not a commitment.

You MUST respond in JSON format with a single key "is_confirmed" (boolean). Example: {{"is_confirmed": true}}
"""
            verification_prompt = verification_prompt_template.format(
                agent_question=current_stage_info.get("prompt", ""),
                user_response=user_input,
                entities=str(entities)
            )
            
            try:
                response = await json_llm.ainvoke([HumanMessage(content=verification_prompt)])
                raw_content = response.content.strip().replace("```json", "").replace("```", "").strip()
                decision = json.loads(raw_content)
                is_confirmed = decision.get("is_confirmed", False)
                
                if is_confirmed:
                    print(f"--- Entity verification PASSED. Updating collected info. ---")
                    collected_info.update({k: v for k, v in entities.items() if v is not None})
                else:
                    print(f"--- Entity verification FAILED. Not updating collected info. ---")
            except Exception as e:
                print(f"Error during entity verification: {e}. Assuming not confirmed.")

        elif entities:
             collected_info.update({k: v for k, v in entities.items() if v is not None})

        print(f"Updated Info: {collected_info}")
    
    # ìŠ¤í…Œì´ì§€ ì „í™˜ ë¡œì§ ê²°ì •
    transitions = current_stage_info.get("transitions", [])
    default_next = current_stage_info.get("default_next_stage_id", "None")
    
    # Case 1: ë¶„ê¸°ê°€ ì—†ëŠ” ê²½ìš° (transitionsê°€ ì—†ê±°ë‚˜ 1ê°œ)
    if len(transitions) <= 1:
        # í•„ìš”í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
        expected_info_key = current_stage_info.get("expected_info_key")
        if expected_info_key and expected_info_key not in collected_info:
            # í•„ìš”í•œ ì •ë³´ê°€ ì•„ì§ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í˜„ì¬ ìŠ¤í…Œì´ì§€ ìœ ì§€
            next_stage_id = current_stage_id
            print(f"--- ìë™ ì§„í–‰ ì°¨ë‹¨: '{expected_info_key}' ì •ë³´ ë¯¸ìˆ˜ì§‘ ---")
        elif len(transitions) == 1:
            # ë‹¨ì¼ ì „í™˜ ê²½ë¡œê°€ ìˆìœ¼ë©´ ìë™ ì§„í–‰
            next_stage_id = transitions[0].get("next_stage_id", default_next)
            print(f"--- ìë™ ì§„í–‰: ë‹¨ì¼ ê²½ë¡œ '{current_stage_id}' â†’ '{next_stage_id}' ---")
        else:
            # transitionsì´ ì—†ìœ¼ë©´ defaultë¡œ ì§„í–‰
            next_stage_id = default_next
            print(f"--- ìë™ ì§„í–‰: ê¸°ë³¸ ê²½ë¡œ '{current_stage_id}' â†’ '{next_stage_id}' ---")
    
    # Case 2: ë¶„ê¸°ê°€ ìˆëŠ” ê²½ìš° (transitionsê°€ 2ê°œ ì´ìƒ) - LLM íŒë‹¨
    else:
        print(f"--- LLM íŒë‹¨ í•„ìš”: {len(transitions)}ê°œ ë¶„ê¸° ì¡´ì¬ ---")
        prompt_template = ALL_PROMPTS.get('main_agent', {}).get('determine_next_scenario_stage', '')
        llm_prompt = prompt_template.format(
            active_scenario_name=active_scenario_data.get("scenario_name"),
            current_stage_id=str(current_stage_id),
            current_stage_prompt=current_stage_info.get("prompt", "No prompt"),
            user_input=state.get("stt_result", ""),
            scenario_agent_intent=scenario_output.get("intent", "N/A") if scenario_output else "N/A",
            scenario_agent_entities=str(scenario_output.get("entities", {}) if scenario_output else {}),
            collected_product_info=str(collected_info),
            formatted_transitions=format_transitions_for_prompt(transitions, current_stage_info.get("prompt", "")),
            default_next_stage_id=default_next
        )
        response = await json_llm.ainvoke([HumanMessage(content=llm_prompt)])
        decision_data = next_stage_decision_parser.parse(response.content)
        next_stage_id = decision_data.chosen_next_stage_id

    # --- ë¡œì§ ì „ìš© ìŠ¤í…Œì´ì§€ ì²˜ë¦¬ ë£¨í”„ ---
    while True:
        if not next_stage_id or str(next_stage_id).startswith("END"):
            break  # ì¢…ë£Œ ìƒíƒœì— ë„ë‹¬í•˜ë©´ ë£¨í”„ íƒˆì¶œ

        next_stage_info = active_scenario_data.get("stages", {}).get(str(next_stage_id), {})
        
        # ìŠ¤í…Œì´ì§€ì— `prompt`ê°€ ìˆìœ¼ë©´ 'ë§í•˜ëŠ” ìŠ¤í…Œì´ì§€'ë¡œ ê°„ì£¼í•˜ê³  ë£¨í”„ íƒˆì¶œ
        if next_stage_info.get("prompt"):
            break
        
        # `prompt`ê°€ ì—†ëŠ” ë¡œì§ ì „ìš© ìŠ¤í…Œì´ì§€ì¸ ê²½ìš°, ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
        print(f"--- Logic Stage Detected: '{next_stage_id}'. Resolving next step automatically. ---")
        
        current_stage_id_for_prompt = str(next_stage_id)
        
        # ë£¨í”„ ë‚´ì—ì„œ prompt_template ì¬ì„¤ì •
        prompt_template = ALL_PROMPTS.get('main_agent', {}).get('determine_next_scenario_stage', '')
        llm_prompt = prompt_template.format(
            active_scenario_name=active_scenario_data.get("scenario_name"),
            current_stage_id=current_stage_id_for_prompt,
            current_stage_prompt=next_stage_info.get("prompt", "No prompt"),
            user_input="<NO_USER_INPUT_PROCEED_AUTOMATICALLY>", # ì‚¬ìš©ì ì…ë ¥ì´ ì—†ìŒì„ ëª…ì‹œ
            scenario_agent_intent="automatic_transition",
            scenario_agent_entities=str({}),
            collected_product_info=str(collected_info),
            formatted_transitions=format_transitions_for_prompt(next_stage_info.get("transitions", []), next_stage_info.get("prompt", "")),
            default_next_stage_id=next_stage_info.get("default_next_stage_id", "None")
        )
        response = await json_llm.ainvoke([HumanMessage(content=llm_prompt)])
        decision_data = next_stage_decision_parser.parse(response.content)
        
        next_stage_id = decision_data.chosen_next_stage_id # ë‹¤ìŒ ìŠ¤í…Œì´ì§€ IDë¥¼ ê°±ì‹ í•˜ê³  ë£¨í”„ ê³„ì†

    # ìµœì¢…ì ìœ¼ë¡œ ê²°ì •ëœ 'ë§í•˜ëŠ”' ìŠ¤í…Œì´ì§€ ID
    determined_next_stage_id = next_stage_id
    
    updated_plan = state.get("action_plan", []).copy()
    if updated_plan:
        updated_plan.pop(0)
    
    updated_struct = state.get("action_plan_struct", []).copy()
    if updated_struct:
        updated_struct.pop(0)

    return {
        **state, 
        "collected_product_info": collected_info, 
        "current_scenario_stage_id": determined_next_stage_id,
        "action_plan": updated_plan,
        "action_plan_struct": updated_struct
    }

# synthesize_response_node is now imported from .nodes.control.synthesize

# end_conversation_node is now imported from .nodes.control.end_conversation

# set_product_type_node is now imported from .nodes.control.set_product
    

# route_after_scenario_logic and execute_plan_router are now imported from .router

# --- Orchestration-Worker Graph Build ---
workflow = StateGraph(AgentState)

# Core Orchestrator
workflow.add_node("entry_point_node", entry_point_node)
workflow.add_node("main_agent_router_node", main_agent_router_node)

# Specialized Workers
workflow.add_node("scenario_worker", call_scenario_agent_node)
workflow.add_node("scenario_flow_worker", process_scenario_logic_node) 
workflow.add_node("rag_worker", factual_answer_node)
workflow.add_node("web_worker", web_search_node)

# Response & Control Nodes
workflow.add_node("synthesize_response_node", synthesize_response_node)
workflow.add_node("set_product_type_node", set_product_type_node)
workflow.add_node("end_conversation_node", end_conversation_node)

# Orchestrator Flow
workflow.set_entry_point("entry_point_node")
workflow.add_edge("entry_point_node", "main_agent_router_node")

# Orchestrator to Workers
workflow.add_conditional_edges(
    "main_agent_router_node",
    execute_plan_router,
    {
        "scenario_worker": "scenario_worker",
        "rag_worker": "rag_worker", 
        "web_worker": "web_worker",
        "synthesize_response_node": "synthesize_response_node",
        "set_product_type_node": "set_product_type_node",
        "end_conversation_node": "end_conversation_node",
    }
)

# Worker Flows
workflow.add_edge("scenario_worker", "scenario_flow_worker")
workflow.add_conditional_edges("scenario_flow_worker", execute_plan_router)
workflow.add_conditional_edges("rag_worker", execute_plan_router)
workflow.add_conditional_edges("web_worker", execute_plan_router)

workflow.add_edge("synthesize_response_node", END)
workflow.add_edge("set_product_type_node", END)
workflow.add_edge("end_conversation_node", END)

app_graph = workflow.compile()
print("--- LangGraph compiled successfully (Orchestration-Worker Architecture). ---")

# --- Backward Compatibility Exports ---
# í…ŒìŠ¤íŠ¸ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì„ì‹œë¡œ ë…¸ë“œ í•¨ìˆ˜ë“¤ì„ re-export
__all__ = [
    'entry_point_node', 
    'main_agent_router_node', 
    'synthesize_response_node',
    'set_product_type_node',
    'end_conversation_node',
    'execute_plan_router',
    'route_after_scenario_logic',
    'app_graph', 
    'run_agent_streaming'
]

async def run_agent_streaming(
    user_input_text: Optional[str] = None,
    user_input_audio_b64: Optional[str] = None,
    session_id: Optional[str] = "default_session",
    current_state_dict: Optional[Dict[str, Any]] = None
) -> AsyncGenerator[Union[Dict[str, Any], str], None]:
    
    if not OPENAI_API_KEY or not json_llm or not generative_llm:
        error_msg = "LLM service is not initialized. Please check API key."
        yield {"type": "error", "message": error_msg}
        yield {"type": "final_state", "data": {"error_message": error_msg, "is_final_turn_response": True}}
        return

    initial_state = cast(AgentState, {
        "session_id": session_id or "default_session",
        "user_input_text": user_input_text,
        "user_input_audio_b64": user_input_audio_b64,
        "messages": current_state_dict.get("messages", []) if current_state_dict else [],
        "current_product_type": current_state_dict.get("current_product_type") if current_state_dict else None,
        "current_scenario_stage_id": current_state_dict.get("current_scenario_stage_id") if current_state_dict else None,
        "collected_product_info": current_state_dict.get("collected_product_info", {}) if current_state_dict else {},
        "available_product_types": ["didimdol", "jeonse", "deposit_account"],
        "action_plan": [],
        "action_plan_struct": [],
        # ì‹œë‚˜ë¦¬ì˜¤ ì—°ì†ì„± ìƒíƒœ ë³µì›
        "scenario_ready_for_continuation": current_state_dict.get("scenario_ready_for_continuation", False) if current_state_dict else False,
        "scenario_awaiting_user_response": current_state_dict.get("scenario_awaiting_user_response", False) if current_state_dict else False,
    })

    print(f"\nğŸš€ ===== AGENT FLOW START [{session_id}] =====")
    log_node_execution("Session", f"product={initial_state['current_product_type']}, input='{user_input_text[:30]}...'")

    final_state: Optional[AgentState] = None
    streamed_text = ""

    try:
        final_state = await app_graph.ainvoke(initial_state)
        
        if final_state and final_state.get("final_response_text_for_tts"):
            text_to_stream = final_state["final_response_text_for_tts"]
            yield {"type": "stream_start"}
            for char in text_to_stream:
                yield char
                streamed_text += char
                await asyncio.sleep(0.01)
            yield {"type": "stream_end", "full_text": streamed_text}
        else:
            error_msg = final_state.get("error_message", "Failed to generate a response.")
            yield {"type": "error", "message": error_msg}
            if final_state: final_state["final_response_text_for_tts"] = error_msg

    except Exception as e:
        print(f"CRITICAL error in run_agent_streaming for session {session_id}: {e}")
        traceback.print_exc()
        error_response = "A critical system error occurred during processing."
        yield {"type": "error", "message": error_response}
        final_state = cast(AgentState, initial_state.copy())
        final_state["error_message"] = error_response
        final_state["is_final_turn_response"] = True
        final_state["messages"] = list(initial_state.get("messages", [])) + [AIMessage(content=error_response)]
    
    finally:
        if final_state:
            yield {"type": "final_state", "data": final_state}
        else:
            final_state = initial_state
            final_state["error_message"] = "Agent execution failed critically, no final state produced."
            final_state["is_final_turn_response"] = True
            yield {"type": "final_state", "data": final_state}
        print(f"ğŸ ===== AGENT FLOW END [{session_id}] =====")
"""
ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤
- ì‹œë‚˜ë¦¬ì˜¤ ë¡œì§ ì²˜ë¦¬, ì •ë³´ ìˆ˜ì§‘, ìŠ¤í…Œì´ì§€ ì „í™˜ ë“±
"""

import json
from typing import Dict, Any, List, Optional
from langchain_core.messages import HumanMessage
from langchain.output_parsers import PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

from ..core.config import get_llm_model
from ..config.prompt_loader import ALL_PROMPTS
from ..services.service_selection_analyzer import service_selection_analyzer
from ..graph.chains import scenario_agent_chain, ScenarioAgentOutput
from .agent_utils import (
    extract_multiple_info_from_text,
    check_required_info_completion,
    generate_missing_info_prompt,
    get_next_missing_info_group_stage,
    generate_group_specific_prompt,
    format_transitions_for_prompt,
    get_active_scenario_data
)
from ..graph.state import AgentState

# json_llm ì´ˆê¸°í™”
json_llm = get_llm_model(response_format={"type": "json_object"})


class NextStageDecision(BaseModel):
    """ë‹¤ìŒ ìŠ¤í…Œì´ì§€ ê²°ì • ëª¨ë¸"""
    chosen_next_stage_id: str = Field(description="ì„ íƒëœ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ ID")
    reasoning: str = Field(description="ê²°ì • ì´ìœ ")


next_stage_decision_parser = PydanticOutputParser(pydantic_object=NextStageDecision)


async def process_multiple_info_collection(
    state: AgentState, 
    active_scenario_data: Dict, 
    current_stage_id: str, 
    current_stage_info: Dict, 
    collected_info: Dict, 
    user_input: str
) -> AgentState:
    """ë‹¤ì¤‘ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬ (ê°œì„ ëœ ê·¸ë£¹ë³„ ë°©ì‹)"""
    required_fields = active_scenario_data.get("required_info_fields", [])
    
    # í˜„ì¬ ìŠ¤í…Œì´ì§€ê°€ ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ì¸ì§€ í™•ì¸
    print(f"í˜„ì¬ ìŠ¤í…Œì´ì§€ ID: {current_stage_id}")
    if current_stage_id in ["info_collection_guidance", "process_collected_info", "ask_missing_info_group1", "ask_missing_info_group2", "ask_missing_info_group3", "eligibility_assessment"]:
        
        # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì •ë³´ ì¶”ì¶œ
        if user_input:
            extracted_info = await extract_multiple_info_from_text(user_input, required_fields)
            print(f"LLM ê¸°ë°˜ ì¶”ì¶œëœ ì •ë³´: {extracted_info}")
            
            # ì‹œë‚˜ë¦¬ì˜¤ ì—ì´ì „íŠ¸ ê²°ê³¼ë„ í™œìš©
            scenario_output = state.get("scenario_agent_output", {})
            if scenario_output and scenario_output.get("entities"):
                scenario_entities = scenario_output["entities"]
                print(f"ì‹œë‚˜ë¦¬ì˜¤ ì—ì´ì „íŠ¸ ì¶”ì¶œ ì •ë³´: {scenario_entities}")
                
                # extracted_infoì— ì—†ëŠ” ì •ë³´ë§Œ ì¶”ê°€
                for key, value in scenario_entities.items():
                    if key not in extracted_info and value is not None:
                        extracted_info[key] = value
                
                # íŠ¹ë³„ ì²˜ë¦¬: í˜¼ì¸ìƒíƒœ
                if "marital_status" in scenario_entities:
                    extracted_info["marital_status"] = scenario_entities["marital_status"]
                    print(f"ì‹œë‚˜ë¦¬ì˜¤ ì—ì´ì „íŠ¸ì—ì„œ í˜¼ì¸ìƒíƒœ í™•ì¸: {scenario_entities['marital_status']}")
            
            # ìˆ˜ì§‘ëœ ì •ë³´ ì—…ë°ì´íŠ¸
            collected_info.update(extracted_info)
            print(f"ìµœì¢… ì—…ë°ì´íŠ¸ëœ ìˆ˜ì§‘ ì •ë³´: {collected_info}")
        
        # ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
        is_complete, missing_fields = check_required_info_completion(collected_info, required_fields)
        
        if current_stage_id == "info_collection_guidance":
            # ì´ˆê¸° ì •ë³´ ì•ˆë‚´ í›„ ë°”ë¡œ ë‹¤ìŒ ê·¸ë£¹ ì§ˆë¬¸ ê²°ì •
            if is_complete:
                next_stage_id = "eligibility_assessment"
            else:
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
            
            # ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±
            customized_prompt = generate_group_specific_prompt(next_stage_id, collected_info)
            state["final_response_text_for_tts"] = customized_prompt
        
        elif current_stage_id == "process_collected_info":
            # ì •ë³´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì¤‘ ë™ì  ë¼ìš°íŒ…
            if is_complete:
                next_stage_id = "eligibility_assessment"
            else:
                next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                customized_prompt = generate_group_specific_prompt(next_stage_id, collected_info)
                state["final_response_text_for_tts"] = customized_prompt
        
        elif current_stage_id.startswith("ask_missing_info_group"):
            # ê·¸ë£¹ë³„ ì§ˆë¬¸ ì²˜ë¦¬
            if user_input:
                # ë‹¤ìŒ ê·¸ë£¹ ë˜ëŠ” í‰ê°€ë¡œ ì´ë™
                if is_complete:
                    next_stage_id = "eligibility_assessment"
                else:
                    next_stage_id = get_next_missing_info_group_stage(collected_info, required_fields)
                    customized_prompt = generate_group_specific_prompt(next_stage_id, collected_info)
                    
                    # ì´ë¯¸ ì„¤ì •ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ì„ ë•Œë§Œ ì„¤ì •
                    if not state.get("final_response_text_for_tts"):
                        state["final_response_text_for_tts"] = customized_prompt
            else:
                # ì‚¬ìš©ì ì…ë ¥ì´ ì—†ìœ¼ë©´ í˜„ì¬ ìŠ¤í…Œì´ì§€ ìœ ì§€
                next_stage_id = current_stage_id
        
        elif current_stage_id == "eligibility_assessment":
            # ìê²© í‰ê°€ëŠ” ë³„ë„ ì²˜ë¦¬
            next_stage_id = current_stage_info.get("next_stage", "loan_recommendation")
        
        else:
            # ê¸°ë³¸ ë‹¤ìŒ ìŠ¤í…Œì´ì§€
            next_stage_id = current_stage_info.get("next_stage", "END")
    
    else:
        # ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ê°€ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ ì‚¬ìš©
        next_stage_id = current_stage_info.get("next_stage", "END")
    
    # ì•¡ì…˜ í”Œëœ ì—…ë°ì´íŠ¸
    updated_plan = state.get("action_plan", []).copy()
    if updated_plan:
        updated_plan.pop(0)
    
    updated_struct = state.get("action_plan_struct", []).copy()
    if updated_struct:
        updated_struct.pop(0)
    
    return {
        **state,
        "collected_product_info": collected_info,
        "current_scenario_stage_id": next_stage_id,
        "action_plan": updated_plan,
        "action_plan_struct": updated_struct
    }


async def process_single_info_collection(
    state: AgentState, 
    active_scenario_data: Dict, 
    current_stage_id: str, 
    current_stage_info: Dict, 
    collected_info: Dict, 
    scenario_output: Optional[ScenarioAgentOutput], 
    user_input: str
) -> AgentState:
    """ê¸°ì¡´ ë‹¨ì¼ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬ (LLM ê¸°ë°˜ ì„œë¹„ìŠ¤ ì„ íƒ ë¶„ì„ í¬í•¨)"""

    if scenario_output and scenario_output.get("is_scenario_related"):
        entities = scenario_output.get("entities", {})
        intent = scenario_output.get("intent", "")
        
        # ğŸ”¥ LLM ê¸°ë°˜ ë¶€ê°€ì„œë¹„ìŠ¤ ì„ íƒ ë¶„ì„ (ì…ì¶œê¸ˆí†µì¥ ì‹œë‚˜ë¦¬ì˜¤ ì „ìš©)
        if (current_stage_id in ["greeting_deposit", "clarify_services"] and 
            "additional_services_choice" in entities and 
            user_input and
            active_scenario_data.get("scenario_name") == "ì‹ í•œì€í–‰ ì…ì¶œê¸ˆí†µì¥ ì‹ ê·œ ìƒë‹´"):
            
            print(f"ğŸ”¥ [LLM-based Service Analysis] Processing input: '{user_input}'")
            
            try:
                # LLM ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰
                normalized_value, next_stage_id, processing_info = await service_selection_analyzer.process_additional_services_input(
                    user_input=user_input,
                    collected_info=collected_info
                )
                
                print(f"ğŸ”¥ [LLM Analysis] Result: value='{normalized_value}', next_stage='{next_stage_id}'")
                print(f"ğŸ”¥ [LLM Analysis] Confidence: {processing_info.get('confidence', 0.0)}")
                
                if normalized_value:
                    # ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    entities["additional_services_choice"] = normalized_value
                    print(f"ğŸ”¥ [LLM Analysis] Updated entity: additional_services_choice = '{normalized_value}'")
                else:
                    # ëª…í™•í™”ê°€ í•„ìš”í•œ ê²½ìš° entitiesì—ì„œ ì œê±°
                    entities.pop("additional_services_choice", None)
                    print(f"ğŸ”¥ [LLM Analysis] Unclear choice, entity removed for clarification")
                
                # ì²˜ë¦¬ ì •ë³´ë¥¼ ìƒíƒœì— ì €ì¥ (ë””ë²„ê¹…ìš©)
                state["llm_service_analysis"] = processing_info
                
            except Exception as e:
                print(f"ğŸ”¥ [LLM Analysis] Error: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
        
        if entities and user_input:
            print(f"--- Verifying extracted entities: {entities} ---")
            verification_prompt_template = """
You are an exceptionally discerning assistant tasked with interpreting a user's intent. Your goal is to determine if the user has made a definitive choice or is simply asking a question about an option.

Here is the conversational context:
- The agent asked the user: "{agent_question}"
- The user replied: "{user_response}"
- From the user's reply, the following information was extracted: {entities}

Your task is to analyze the user's reply carefully. Has the user **committed** to the choice represented by the extracted information?

Consider these rules:
1.  **Direct questions are not commitments.** If the user asks "What is [option]?" or "Are there fees for [option]?", they have NOT committed.
2.  **Hypotheticals can be commitments.** If the user asks "If I choose [option], what happens next?", they ARE committing to that option for the sake of continuing the conversation.
3.  **Ambiguity means no commitment.** If it's unclear, err on the side of caution and decide it's not a commitment.

You MUST respond in JSON format with a single key "is_confirmed" (boolean). Example: {{"is_confirmed": true}}
"""
            verification_prompt = verification_prompt_template.format(
                agent_question=current_stage_info.get("prompt", ""),
                user_response=user_input,
                entities=str(entities)
            )
            
            try:
                response = await json_llm.ainvoke([HumanMessage(content=verification_prompt)])
                raw_content = response.content.strip().replace("```json", "").replace("```", "").strip()
                decision = json.loads(raw_content)
                is_confirmed = decision.get("is_confirmed", False)
                
                if is_confirmed:
                    print(f"--- Entity verification PASSED. Updating collected info. ---")
                    # deposit_account ì‹œë‚˜ë¦¬ì˜¤ì˜ ê²½ìš° ì—”í‹°í‹° ë§¤í•‘ ì ìš©
                    if state.get("current_product_type") == "deposit_account":
                        entity_mapping = {
                            "name": "customer_name",
                            "contact": "phone_number"
                        }
                        mapped_entities = {}
                        for k, v in entities.items():
                            mapped_key = entity_mapping.get(k, k)
                            if v is not None:
                                mapped_entities[mapped_key] = v
                                print(f"Mapping: {k} -> {mapped_key} = {v}")
                        collected_info.update(mapped_entities)
                        print(f"Mapped entities: {mapped_entities}")
                    else:
                        collected_info.update({k: v for k, v in entities.items() if v is not None})
                else:
                    print(f"--- Entity verification FAILED. Not updating collected info. ---")
            except Exception as e:
                print(f"Error during entity verification: {e}. Assuming not confirmed.")

        elif entities:
            # deposit_account ì‹œë‚˜ë¦¬ì˜¤ì˜ ê²½ìš° ì—”í‹°í‹° ë§¤í•‘
            if state.get("current_product_type") == "deposit_account":
                entity_mapping = {
                    "name": "customer_name",
                    "contact": "phone_number"
                }
                mapped_entities = {}
                for k, v in entities.items():
                    mapped_key = entity_mapping.get(k, k)
                    if v is not None:
                        mapped_entities[mapped_key] = v
                collected_info.update(mapped_entities)
            else:
                collected_info.update({k: v for k, v in entities.items() if v is not None})
        
        # deposit_accountì˜ yes/no ì§ˆë¬¸ì— ëŒ€í•œ LLM ê¸°ë°˜ ì²˜ë¦¬
        if (state.get("current_product_type") == "deposit_account" and 
            current_stage_id == "collect_basic" and 
            "use_lifelong_account" not in collected_info and
            len(collected_info) >= 2):
            
            print(f"ğŸ” [LLM-based Analysis] Processing yes/no response for use_lifelong_account")
            
            # LLMì„ í†µí•œ yes/no ë¶„ì„
            yes_no_prompt = f"""
ì‚¬ìš©ìì—ê²Œ í‰ìƒê³„ì¢Œ ì„œë¹„ìŠ¤ ì‚¬ìš© ì—¬ë¶€ë¥¼ ë¬¼ì–´ë´¤ê³ , ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í–ˆìŠµë‹ˆë‹¤:
"{user_input}"

ì´ ë‹µë³€ì´ ê¸ì •(ì˜ˆ/ë™ì˜)ì¸ì§€ ë¶€ì •(ì•„ë‹ˆì˜¤/ê±°ë¶€)ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "is_positive": true/false,
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±°"
}}
"""
            try:
                response = await json_llm.ainvoke([HumanMessage(content=yes_no_prompt)])
                result = json.loads(response.content.strip().replace("```json", "").replace("```", ""))
                
                is_positive = result.get("is_positive", None)
                confidence = result.get("confidence", 0.0)
                reasoning = result.get("reasoning", "")
                
                print(f"ğŸ” [LLM Analysis] Result: {'ê¸ì •' if is_positive else 'ë¶€ì •'}, Confidence: {confidence}")
                print(f"ğŸ” [LLM Analysis] Reasoning: {reasoning}")
                
                if confidence >= 0.7 and is_positive is not None:
                    collected_info["use_lifelong_account"] = is_positive
                    print(f"í‰ìƒê³„ì¢Œ ì‚¬ìš© {'ë™ì˜' if is_positive else 'ê±°ë¶€'}ë¡œ í•´ì„: use_lifelong_account = {is_positive}")
                else:
                    print(f"ğŸ” [LLM Analysis] Confidence too low ({confidence}), skipping field update")
                    
            except Exception as e:
                print(f"ğŸ” [LLM Analysis] Error: {e}")

        print(f"Updated Info: {collected_info}")
    
    # slot_filling íƒ€ì… ìŠ¤í…Œì´ì§€ ì²˜ë¦¬
    if current_stage_info.get("type") == "slot_filling":
        print(f"--- Slot Filling Stage: {current_stage_id} ---")
        required_fields = current_stage_info.get("required_fields", [])
        
        # í˜„ì¬ ìŠ¤í…Œì´ì§€ì˜ í•„ìˆ˜ í•„ë“œë§Œ í™•ì¸
        stage_fields = []
        all_fields = active_scenario_data.get("slot_fields", [])
        for field in all_fields:
            if field["key"] in required_fields:
                stage_fields.append(field)
        
        # í•„ìˆ˜ í•„ë“œ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
        all_collected = True
        for field_key in required_fields:
            if field_key not in collected_info:
                all_collected = False
                break
        
        print(f"Required fields: {required_fields}")
        print(f"Collected info keys: {list(collected_info.keys())}")
        print(f"Collected info full: {collected_info}")
        print(f"All collected: {all_collected}")
        
        if all_collected:
            # ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©´ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ë¡œ ì§„í–‰
            determined_next_stage_id = current_stage_info.get("next_stage", "END")
            print(f"All required fields collected. Moving to next stage: {determined_next_stage_id}")
        else:
            # ì•„ì§ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í•„ë“œê°€ ìˆìœ¼ë©´ í˜„ì¬ ìŠ¤í…Œì´ì§€ ìœ ì§€
            determined_next_stage_id = current_stage_id
            print(f"Missing some required fields. Staying at current stage: {current_stage_id}")
        
        # ì‘ë‹µ ë©”ì‹œì§€ ìƒì„±
        if all_collected:
            # ëª¨ë“  í•„ë“œê°€ ìˆ˜ì§‘ë˜ë©´ ì™„ë£Œ ë©”ì‹œì§€
            state["final_response_text_for_tts"] = current_stage_info.get("completion_message", "ê¸°ë³¸ ì •ë³´ í™•ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            # ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í•„ë“œì— ë”°ë¥¸ ë©”ì‹œì§€
            if "customer_name" not in collected_info or "phone_number" not in collected_info:
                # ì´ë¦„ì´ë‚˜ ì—°ë½ì²˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
                state["final_response_text_for_tts"] = current_stage_info.get("message", "")
            elif "use_lifelong_account" not in collected_info:
                # ì´ë¦„ê³¼ ì—°ë½ì²˜ëŠ” ìˆì§€ë§Œ í‰ìƒê³„ì¢Œ ì‚¬ìš© ì—¬ë¶€ê°€ ì—†ìœ¼ë©´
                state["final_response_text_for_tts"] = "í‰ìƒê³„ì¢Œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì‹œê² ì–´ìš”? íœ´ëŒ€í°ë²ˆí˜¸ë¥¼ ê³„ì¢Œë²ˆí˜¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ í¸ë¦¬í•©ë‹ˆë‹¤."
    else:
        # ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬
        determined_next_stage_id = None
    
    # ê¸°ì¡´ LLM ê¸°ë°˜ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ ê²°ì • (slot_fillingì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
    if determined_next_stage_id is None:
        # ë¨¼ì € LLMì„ í†µí•´ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ë¥¼ ê²°ì •
        prompt_template = ALL_PROMPTS.get('main_agent', {}).get('determine_next_scenario_stage', '')
        llm_prompt = prompt_template.format(
            active_scenario_name=active_scenario_data.get("scenario_name"),
            current_stage_id=str(current_stage_id),
            current_stage_prompt=current_stage_info.get("prompt", "No prompt"),
            user_input=state.get("stt_result", ""),
            scenario_agent_intent=scenario_output.get("intent", "N/A"),
            scenario_agent_entities=str(scenario_output.get("entities", {})),
            collected_product_info=str(collected_info),
            formatted_transitions=format_transitions_for_prompt(current_stage_info.get("transitions", []), current_stage_info.get("prompt", "")),
            default_next_stage_id=current_stage_info.get("default_next_stage_id", "None")
        )
        response = await json_llm.ainvoke([HumanMessage(content=llm_prompt)])
        decision_data = next_stage_decision_parser.parse(response.content)
        next_stage_id = decision_data.chosen_next_stage_id
    else:
        # slot_fillingì—ì„œ ì´ë¯¸ ê²°ì •ëœ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ ì‚¬ìš©
        next_stage_id = determined_next_stage_id

    # --- ë¡œì§ ì „ìš© ìŠ¤í…Œì´ì§€ ì²˜ë¦¬ ë£¨í”„ ---
    while True:
        if not next_stage_id or str(next_stage_id).startswith("END"):
            break  # ì¢…ë£Œ ìƒíƒœì— ë„ë‹¬í•˜ë©´ ë£¨í”„ íƒˆì¶œ

        next_stage_info = active_scenario_data.get("stages", {}).get(str(next_stage_id), {})
        
        # ìŠ¤í…Œì´ì§€ì— `prompt` ë˜ëŠ” `message`ê°€ ìˆìœ¼ë©´ 'ë§í•˜ëŠ” ìŠ¤í…Œì´ì§€'ë¡œ ê°„ì£¼í•˜ê³  ë£¨í”„ íƒˆì¶œ
        if next_stage_info.get("prompt") or next_stage_info.get("message"):
            break
        
        # `prompt`ê°€ ì—†ëŠ” ë¡œì§ ì „ìš© ìŠ¤í…Œì´ì§€ì¸ ê²½ìš°, ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
        print(f"--- Logic Stage Detected: '{next_stage_id}'. Resolving next step automatically. ---")
        
        current_stage_id_for_prompt = str(next_stage_id)
        
        llm_prompt = prompt_template.format(
            active_scenario_name=active_scenario_data.get("scenario_name"),
            current_stage_id=current_stage_id_for_prompt,
            current_stage_prompt=next_stage_info.get("prompt", "No prompt"),
            user_input="<NO_USER_INPUT_PROCEED_AUTOMATICALLY>", # ì‚¬ìš©ì ì…ë ¥ì´ ì—†ìŒì„ ëª…ì‹œ
            scenario_agent_intent="automatic_transition",
            scenario_agent_entities=str({}),
            collected_product_info=str(collected_info),
            formatted_transitions=format_transitions_for_prompt(next_stage_info.get("transitions", []), next_stage_info.get("prompt", "")),
            default_next_stage_id=next_stage_info.get("default_next_stage_id", "None")
        )
        response = await json_llm.ainvoke([HumanMessage(content=llm_prompt)])
        decision_data = next_stage_decision_parser.parse(response.content)
        
        next_stage_id = decision_data.chosen_next_stage_id # ë‹¤ìŒ ìŠ¤í…Œì´ì§€ IDë¥¼ ê°±ì‹ í•˜ê³  ë£¨í”„ ê³„ì†

    # ìµœì¢…ì ìœ¼ë¡œ ê²°ì •ëœ 'ë§í•˜ëŠ”' ìŠ¤í…Œì´ì§€ ID
    determined_next_stage_id = next_stage_id
    
    updated_plan = state.get("action_plan", []).copy()
    if updated_plan:
        updated_plan.pop(0)
    
    updated_struct = state.get("action_plan_struct", []).copy()
    if updated_struct:
        updated_struct.pop(0)

    return {
        **state, 
        "collected_product_info": collected_info, 
        "current_scenario_stage_id": determined_next_stage_id,
        "action_plan": updated_plan,
        "action_plan_struct": updated_struct
    }


async def process_scenario_logic_node(state: AgentState) -> AgentState:
    """ì‹œë‚˜ë¦¬ì˜¤ ë¡œì§ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜"""
    print("--- Node: Process Scenario Logic ---")
    
    active_scenario_data = get_active_scenario_data(state)
    current_stage_id = state.get("current_scenario_stage_id")
    
    # ìŠ¤í…Œì´ì§€ IDê°€ ì—†ëŠ” ê²½ìš° ì´ˆê¸° ìŠ¤í…Œì´ì§€ë¡œ ì„¤ì •
    if not current_stage_id:
        current_stage_id = active_scenario_data.get("initial_stage_id", "greeting")
        print(f"ìŠ¤í…Œì´ì§€ IDê°€ ì—†ì–´ì„œ ì´ˆê¸° ìŠ¤í…Œì´ì§€ë¡œ ì„¤ì •: {current_stage_id}")
    
    current_stage_info = active_scenario_data.get("stages", {}).get(str(current_stage_id), {})
    print(f"í˜„ì¬ ìŠ¤í…Œì´ì§€: {current_stage_id}, ìŠ¤í…Œì´ì§€ ì •ë³´: {current_stage_info.keys()}")
    collected_info = state.get("collected_product_info", {}).copy()
    scenario_output = state.get("scenario_agent_output")
    user_input = state.get("stt_result", "")
    
    # ê°œì„ ëœ ë‹¤ì¤‘ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬
    print(f"ìŠ¤í…Œì´ì§€ ì •ë³´ í™•ì¸ - collect_multiple_info: {current_stage_info.get('collect_multiple_info')}")
    if current_stage_info.get("collect_multiple_info"):
        print("--- ë‹¤ì¤‘ ì •ë³´ ìˆ˜ì§‘ ëª¨ë“œ ---")
        return await process_multiple_info_collection(state, active_scenario_data, current_stage_id, current_stage_info, collected_info, user_input)
    
    # ê¸°ì¡´ ë‹¨ì¼ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬
    return await process_single_info_collection(state, active_scenario_data, current_stage_id, current_stage_info, collected_info, scenario_output, user_input)
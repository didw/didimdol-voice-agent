<script setup lang="ts">
import { ref, onMounted, onBeforeUnmount, watch, nextTick } from 'vue'
import { useChatStore } from '@/stores/chatStore'
import { storeToRefs } from 'pinia'

const chatStore = useChatStore()
const {
  messages,
  isProcessingLLM,
  isSynthesizingTTS,
  error,
  currentInterimStt,
  isWebSocketConnected,
  sessionId,
  getAudioChunks, // TTS ì˜¤ë””ì˜¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
  isEPDDetected,
} = storeToRefs(chatStore)

const userInput = ref('')
const isRecording = ref(false)
let mediaRecorder: MediaRecorder | null = null
let audioChunks: Blob[] = []

// ì˜¤ë””ì˜¤ ì¬ìƒ ê´€ë ¨
const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
let audioQueue: string[] = [] // Base64 ì²­í¬ ì €ì¥ í
let isPlayingQueue = false
let sourceNode: AudioBufferSourceNode | null = null // í˜„ì¬ ì¬ìƒ ì¤‘ì¸ source node

// EPD, Barge-in ê´€ë ¨
const VAD_THRESHOLD = 0.01 // Voice Activity Detection ì„ê³„ê°’ (ì¡°ì • í•„ìš”)
const SILENCE_TIMEOUT_MS = 1500 // EPDë¥¼ ìœ„í•œ ì¹¨ë¬µ ì‹œê°„ (ms)
let silenceTimer: number | null = null
let audioProcessorNode: ScriptProcessorNode | null = null

const messagesContainer = ref<HTMLElement | null>(null)

const scrollToBottom = async () => {
  await nextTick()
  if (messagesContainer.value) {
    messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
  }
}

watch(messages, scrollToBottom, { deep: true })
watch(currentInterimStt, scrollToBottom)

onMounted(() => {
  chatStore.initializeSessionAndConnect()
  scrollToBottom()
})

onBeforeUnmount(() => {
  stopRecording()
  if (audioProcessorNode) {
    audioProcessorNode.disconnect()
  }
  if (audioContext.state !== 'closed') {
    audioContext.close()
  }
  chatStore.disconnectWebSocket()
})

const sendTextMessage = () => {
  if (userInput.value.trim() && isWebSocketConnected.value) {
    chatStore.sendWebSocketTextMessage(userInput.value)
    userInput.value = ''
  } else if (!isWebSocketConnected.value) {
    alert('ì„œë²„ì™€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
  }
}

const toggleRecording = async () => {
  if (!isWebSocketConnected.value) {
    alert('ì„œë²„ì™€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŒì„± ì…ë ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    return
  }
  if (isRecording.value) {
    stopRecording()
  } else {
    await startRecording()
  }
}

const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    isRecording.value = true
    audioChunks = []
    // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ê°€ suspended ìƒíƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ resume ì‹œë„
    if (audioContext.state === 'suspended') {
      await audioContext.resume()
    }

    // MediaRecorder ì„¤ì •
    const options = { mimeType: 'audio/webm;codecs=opus' } // Opus ì½”ë± ì‚¬ìš© ê¶Œì¥
    mediaRecorder = new MediaRecorder(stream, options)

    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.push(event.data)
        chatStore.sendAudioBlob(event.data) // ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì„œë²„ì— ì „ì†¡
      }
    }

    mediaRecorder.onstop = () => {
      isRecording.value = false
      // ë‚¨ì•„ìˆëŠ” ì²­í¬ê°€ ìˆë‹¤ë©´ í•œë²ˆ ë” ì „ì†¡ (ì˜µì…˜)
      // if (audioChunks.length > 0) {
      //   const finalBlob = new Blob(audioChunks, { type: options.mimeType });
      //   chatStore.sendAudioBlob(finalBlob);
      // }
      stream.getTracks().forEach((track) => track.stop()) // ìŠ¤íŠ¸ë¦¼ íŠ¸ë™ ì¤‘ì§€
      console.log('Recording stopped, final chunks sent (if any).')
    }

    mediaRecorder.onerror = (event) => {
      console.error('MediaRecorder error:', event)
      chatStore.error = 'ë…¹ìŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
      isRecording.value = false
    }

    // 100ms ë§ˆë‹¤ ì²­í¬ ì „ì†¡ (ë˜ëŠ” ì„œë²„ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì ˆ)
    mediaRecorder.start(250) // EPD/Barge-inì„ ìœ„í•´ ë” ì§§ì€ ê°„ê²©ìœ¼ë¡œ ì²­í¬ ì „ì†¡
    console.log('Recording started...')

    // EPD ë¡œì§ (í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ VAD - ê°„ë‹¨ ë²„ì „)
    // ë” ì •êµí•œ VADëŠ” Web Audio APIì˜ AnalyserNode ì‚¬ìš© ë˜ëŠ” ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
    const audioSource = audioContext.createMediaStreamSource(stream)
    audioProcessorNode = audioContext.createScriptProcessor(4096, 1, 1) // bufferSize, inputChannels, outputChannels

    audioProcessorNode.onaudioprocess = (e) => {
      if (!isRecording.value) return // ë…¹ìŒ ì¤‘ì´ ì•„ë‹ˆë©´ ì²˜ë¦¬ ì•ˆí•¨

      const inputData = e.inputBuffer.getChannelData(0)
      let sum = 0
      for (let i = 0; i < inputData.length; ++i) {
        sum += inputData[i] * inputData[i]
      }
      const rms = Math.sqrt(sum / inputData.length)

      if (rms > VAD_THRESHOLD) {
        // ìŒì„± ê°ì§€
        if (silenceTimer) {
          clearTimeout(silenceTimer)
          silenceTimer = null
        }
      } else {
        // ì¹¨ë¬µ ê°ì§€
        if (!silenceTimer && isRecording.value) {
          // isRecording.value ì²´í¬ ì¶”ê°€
          silenceTimer = window.setTimeout(() => {
            if (isRecording.value) {
              // íƒ€ì„ì•„ì›ƒ ì‹œì ì—ë„ ë…¹ìŒ ì¤‘ì¸ì§€ ì¬í™•ì¸
              console.log('Client-side EPD: Silence detected, stopping recording.')
              stopRecording() // ì¹¨ë¬µ ê¸¸ì–´ì§€ë©´ ë…¹ìŒ ì¤‘ì§€ (EPD)
            }
          }, SILENCE_TIMEOUT_MS)
        }
      }
    }
    audioSource.connect(audioProcessorNode)
    audioProcessorNode.connect(audioContext.destination) // ì‹¤ì œ ì˜¤ë””ì˜¤ ì¶œë ¥ì€ ì•ˆ í•¨
  } catch (err) {
    console.error('Error starting recording:', err)
    chatStore.error = 'ìŒì„± ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.'
    isRecording.value = false
  }
}

const stopRecording = () => {
  if (mediaRecorder && isRecording.value) {
    mediaRecorder.stop() // onstop í•¸ë“¤ëŸ¬ì—ì„œ isRecording.value = false ì²˜ë¦¬
    console.log('Recording stop requested.')
  }
  if (silenceTimer) {
    clearTimeout(silenceTimer)
    silenceTimer = null
  }
  if (audioProcessorNode) {
    audioProcessorNode.disconnect()
    // audioProcessorNode = null; // í•„ìš”ì‹œ null ì²˜ë¦¬
  }
}

// ì„œë²„ì—ì„œ EPD ê°ì§€ ì‹œ ë…¹ìŒ ì¤‘ì§€
watch(isEPDDetected, (newVal) => {
  if (newVal && isRecording.value) {
    console.log('EPD detected from server, stopping client recording.')
    stopRecording()
    chatStore.isEPDDetected = false // ìƒíƒœ ë‹¤ì‹œ ì´ˆê¸°í™”
  }
})

// TTS ì˜¤ë””ì˜¤ ì²­í¬ ì¬ìƒ ë¡œì§
watch(
  getAudioChunks,
  async (newChunks) => {
    if (newChunks.length > 0) {
      audioQueue.push(...newChunks) // ìƒˆ ì²­í¬ë¥¼ íì— ì¶”ê°€
      chatStore.clearAudioChunks() // ìŠ¤í† ì–´ì˜ ì²­í¬ëŠ” ë¹„ì›€
      if (!isPlayingQueue) {
        playNextChunkFromQueue()
      }
    }
  },
  { deep: true },
)

const playNextChunkFromQueue = async () => {
  if (audioQueue.length === 0) {
    isPlayingQueue = false
    return
  }
  isPlayingQueue = true
  const base64Chunk = audioQueue.shift()

  if (base64Chunk) {
    try {
      const audioData = Uint8Array.from(atob(base64Chunk), (c) => c.charCodeAt(0)).buffer
      const audioBuffer = await audioContext.decodeAudioData(audioData)

      // ì´ì „ sourceNodeê°€ ìˆë‹¤ë©´ ì¤‘ì§€ (Barge-in ëŒ€ë¹„)
      if (sourceNode) {
        sourceNode.stop()
        sourceNode.disconnect()
      }

      sourceNode = audioContext.createBufferSource()
      sourceNode.buffer = audioBuffer
      sourceNode.connect(audioContext.destination)
      sourceNode.start()
      sourceNode.onended = () => {
        if (sourceNode) {
          // onended ì½œë°± ì‹œì ì—ëŠ” sourceNodeê°€ nullì´ ì•„ë‹˜ì„ ë³´ì¥
          sourceNode.disconnect() // ì—°ê²° í•´ì œ
        }
        sourceNode = null // ì¬ìƒ ì™„ë£Œ í›„ nullë¡œ ì„¤ì •
        if (audioQueue.length > 0) {
          playNextChunkFromQueue() // ë‹¤ìŒ ì²­í¬ ì¬ìƒ
        } else {
          isPlayingQueue = false // í ë¹„ë©´ ì¬ìƒ ì¤‘ì§€ ìƒíƒœ
        }
      }
    } catch (e) {
      console.error('Error decoding or playing audio chunk:', e)
      chatStore.error = 'ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
      isPlayingQueue = false
      playNextChunkFromQueue() // ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ì²­í¬ ì‹œë„
    }
  } else {
    playNextChunkFromQueue() // ë¹ˆ ì²­í¬ë©´ ë‹¤ìŒìœ¼ë¡œ
  }
}

// Barge-in: ì‚¬ìš©ì ë°œí™” ì‹œì‘ ì‹œ TTS ì¤‘ë‹¨
const handleUserInputFocus = () => {
  if (isPlayingQueue || isSynthesizingTTS.value) {
    console.log('User input focus, attempting to stop TTS for Barge-in.')
    chatStore.requestStopTTS() // ì„œë²„ì— TTS ì¤‘ë‹¨ ìš”ì²­

    // í´ë¼ì´ì–¸íŠ¸ ì¸¡ì—ì„œë„ ì¦‰ì‹œ ì¤‘ë‹¨
    if (sourceNode) {
      sourceNode.stop()
      sourceNode.disconnect()
      sourceNode = null
    }
    audioQueue = [] // ì˜¤ë””ì˜¤ ì¬ìƒ í ë¹„ìš°ê¸°
    isPlayingQueue = false
  }
}
</script>

<template>
  <div class="chat-container">
    <header class="chat-header">
      <h2>
        ë””ë”¤ëŒ ëŒ€ì¶œ ìŒì„±ë´‡ <small v-if="sessionId"> (ì„¸ì…˜: {{ sessionId?.substring(0, 8) }})</small>
      </h2>
      <div class="status-indicators">
        <span
          :class="['status-dot', isWebSocketConnected ? 'connected' : 'disconnected']"
          :title="isWebSocketConnected ? 'ì„œë²„ ì—°ê²°ë¨' : 'ì„œë²„ ì—°ê²° ëŠê¹€'"
        ></span>
        <span v-if="isRecording" class="status-text recording-active" title="ë…¹ìŒ ì¤‘">REC</span>
        <span v-if="isProcessingLLM" class="status-text" title="AI ìƒê° ì¤‘"
          >AI ì‘ë‹µ ìƒì„± ì¤‘...</span
        >
        <span v-if="isSynthesizingTTS && !isProcessingLLM" class="status-text" title="ìŒì„± í•©ì„± ì¤‘"
          >ìŒì„± ì¤€ë¹„ ì¤‘...</span
        >
      </div>
    </header>

    <div class="messages-area" ref="messagesContainer">
      <div
        v-for="message in messages"
        :key="message.id"
        :class="['message-bubble', message.sender]"
      >
        <p>
          <span v-if="message.sender === 'user'">ğŸ‘¤:</span>
          <span v-else>ğŸ¤–:</span>
          {{ message.text }}
          <span v-if="message.isStreaming" class="streaming-cursor"></span>
        </p>
        <span class="timestamp">{{ new Date(message.timestamp).toLocaleTimeString() }}</span>
      </div>
      <div v-if="currentInterimStt" class="message-bubble user interim-stt">
        <p>ğŸ‘¤: {{ currentInterimStt }}<span class="streaming-cursor"></span></p>
      </div>
    </div>

    <div v-if="error" class="error-message">ì˜¤ë¥˜: {{ error }}</div>

    <div class="input-area">
      <textarea
        v-model="userInput"
        @keyup.enter.exact="sendTextMessage"
        @focus="handleUserInputFocus"
        placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”..."
        :disabled="isRecording || !isWebSocketConnected"
      ></textarea>
      <button
        @click="sendTextMessage"
        :disabled="!userInput.trim() || isRecording || !isWebSocketConnected"
        class="send-button"
      >
        ì „ì†¡
      </button>
      <button
        @click="toggleRecording"
        :class="['mic-button', { recording: isRecording }]"
        :disabled="!isWebSocketConnected"
        :title="isRecording ? 'ë…¹ìŒ ì¤‘ì§€' : 'ë…¹ìŒ ì‹œì‘'"
      >
        ğŸ¤
      </button>
    </div>
  </div>
</template>

<style scoped>
.chat-container {
  display: flex;
  flex-direction: column;
  height: 100%;
  max-width: 800px;
  margin: auto;
  border: 1px solid #ccc;
  border-radius: 8px;
  overflow: hidden;
  background-color: #f9f9f9;
  box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

.chat-header {
  background-color: #4caf50;
  color: white;
  padding: 10px 15px;
  text-align: center;
  display: flex;
  justify-content: space-between;
  align-items: center;
}
.chat-header h2 {
  margin: 0;
  font-size: 1.2em;
}
.status-indicators {
  display: flex;
  align-items: center;
  gap: 8px;
}
.status-dot {
  width: 12px;
  height: 12px;
  border-radius: 50%;
  display: inline-block;
}
.status-dot.connected {
  background-color: #8bc34a; /* Light Green */
}
.status-dot.disconnected {
  background-color: #f44336; /* Red */
}

.status-text {
  font-size: 0.8em;
  padding: 2px 6px;
  border-radius: 4px;
  background-color: rgba(255, 255, 255, 0.2);
}
.recording-active {
  background-color: #ff9800; /* Orange for recording */
  color: white;
}

.messages-area {
  flex-grow: 1;
  overflow-y: auto;
  padding: 15px;
  display: flex;
  flex-direction: column;
  gap: 10px;
  background-color: #ffffff;
}

.message-bubble {
  padding: 10px 15px;
  border-radius: 18px;
  max-width: 75%;
  word-wrap: break-word;
}

.message-bubble.user {
  background-color: #dcf8c6;
  align-self: flex-end;
  margin-left: auto;
  text-align: right;
}

.message-bubble.ai {
  background-color: #ececec;
  align-self: flex-start;
  margin-right: auto;
}
.message-bubble p {
  margin: 0;
  line-height: 1.4;
}

.timestamp {
  display: block;
  font-size: 0.75em;
  color: #888;
  margin-top: 5px;
}
.message-bubble.user .timestamp {
  text-align: right;
}
.message-bubble.ai .timestamp {
  text-align: left;
}

.interim-stt p {
  color: #777;
  font-style: italic;
}

.streaming-cursor::after {
  content: 'â–‹';
  animation: blink 1s step-end infinite;
  font-size: 0.9em;
  margin-left: 2px;
}

@keyframes blink {
  50% {
    opacity: 0;
  }
}

.input-area {
  display: flex;
  padding: 10px;
  border-top: 1px solid #ccc;
  background-color: #f0f0f0;
}

.input-area textarea {
  flex-grow: 1;
  padding: 10px;
  border: 1px solid #ddd;
  border-radius: 20px;
  margin-right: 10px;
  resize: none;
  min-height: 24px; /* ìµœì†Œ ë†’ì´ */
  max-height: 120px; /* ìµœëŒ€ ë†’ì´ */
  overflow-y: auto; /* ë‚´ìš© ë§ì„ ì‹œ ìŠ¤í¬ë¡¤ */
  font-size: 1em;
  line-height: 1.4;
}

.input-area button {
  padding: 10px 15px;
  border: none;
  border-radius: 20px;
  cursor: pointer;
  font-size: 1em;
}
.send-button {
  background-color: #4caf50;
  color: white;
}
.send-button:disabled {
  background-color: #a5d6a7;
}

.mic-button {
  background-color: #2196f3;
  color: white;
  margin-left: 5px;
}
.mic-button.recording {
  background-color: #f44336; /* Red when recording */
}
.mic-button:disabled {
  background-color: #90caf9;
}

.error-message {
  color: red;
  padding: 10px;
  text-align: center;
  font-size: 0.9em;
  background-color: #ffebee;
  border-bottom: 1px solid #e57373;
}
</style>
